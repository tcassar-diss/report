\section{Implementation}\label{sec:implementation}

Unlike the design phase, implementing \af was not done in discrete
stages. A key design choice was to use \ac{bpf} - a challenging programming language
to develop in due to its highly restrictive nature. The lack of accessible
documentation for \ac{bpf} added to these challenges and necessitated a more
iterative approach to implementation.

As such, we do not present this section in chronological order: we begin with
an outline of the technology stack used to build \afss,~and then talk about how
we implemented key processes like finding the system call site. We discuss 
how key security vulnerabilities were avoided and talk about performance 
optimisations built into \af.

\subsection{Technology Stack}\label{subsection:tech-stack}

We decided to use \ac{bpf} in the backend, Go for the frontend, and the
\texttt{ebpf-go} library from the Cilium project to interoperate between Go and
\ac{bpf}. We wrote \af in 3,400 \ac{loc}, 67\% of which was Go, and 33\% was 
\ac{bpf}. We developed around another 1,500\ac{loc} for additional tooling and
almost 3,000 \ac{loc} of Python to analyse benchmark results (as outlined in
Section~\ref{subsec:perf-eval}). Table~\ref{tab:loc_breakdown} summarises these figures.

\begin{table}[htbp] % Placement options: here, top, bottom, page
    \centering % Center the table
    \caption{Breakdown of Source Code Lines of Code (LOC) by Component and Language.} % Table caption
    \label{tab:loc_breakdown} % Label for referencing the table (\ref{tab:loc_breakdown}) - KEPT THE SAME
    \begin{tabular}{l l r l} % Column definitions: left, left, right, left alignment
        \toprule % Top rule from booktabs
        \textbf{Component/Purpose} & \textbf{Language/Technology} & \textbf{$\sim$ Lines of Code (LOC)}  \\
        \midrule % Middle rule from booktabs
        Main Application (\af) & Go                  & 2,500 \\
        Main Application (\af) & BPF (\ac{bpf})    & 850 \\
        \midrule % Optional rule before subtotal
        \textbf{Subtotal: Main App} & \textbf{Go \& BPF} &  \textbf{3,400}\\
        \midrule
        Additional Tooling       & Go/\ac{bpf}     & 1,500 \\
        Benchmark Analysis       & Python              & 3,000                       \\
        \midrule % Rule before grand total
        \textbf{Grand Total} & \textbf{Various} & \textbf{7,900} \\
        \bottomrule % Bottom rule from booktabs
    \end{tabular}
\end{table}

\ac{bpf} was chosen as it is lightweight, secure, and does not require 
patching the kernel. It is expressive enough to be able to achieve 
all of the design requirements listed in Section \ref{subsec:objs-assumpions-tm} 
but also formally verified so that causing the kernel to crash becomes
impossible.\footnote{Exploits and bugs mean that this is not impossible, but in
practice, it is extremely hard to do.} \ac{bpf} programs can also be adopted and
applied to programs entirely transparently: no configuration files need changing
and no business logic needs changing \cite{halinen-security-risks-sidecar-containers-2024}. \ac{bpf} is extremely restrictive, but
this is the cost of not having to patch the kernel/using kernel modules, which, in
the worst case may require recompiling the kernel
\cite{kbuild-modules-linux-kernel-docs-2025}, 
and would make it harder for developers to use \af.

\ac{bpf} programs are commonly written in C and compiled to \ac{bpf} bytecode.
Developers commonly write their program's non-\ac{bpf} components 
in C as structs can be shared between \ac{bpf} and the rest of the code.
This choice of Go may seem slightly unorthodox. We believe that Go's
excellent concurrency model, ease of use, and compatibility with \ac{bpf} via
\texttt{ebpf-go} make it the right choice for this project. 

It is significantly easier to develop in Go than in C owing to its runtime and other 
modern language features. Reading ringbuffers asynchronously is much easier with 
goroutines than with pthreads, and the ability to manage long-lived applications 
via the  ``Context'' concurrency pattern makes Go a strong choice for developing the 
frontend. Go's runtime, of course, adds a performance penalty during execution. This  is not a problem for \afss, as all the code which must be performant is \ac{bpf} 
and therefore is not affected by Go's runtime. 

\texttt{ebpf-go} handles autogeneration of idiomatic Go code from the C/\ac{bpf}
source (with the \texttt{go2bpf} tool),
compilation of the \ac{bpf} bytecode, and provides mechanisms to attach programs
to tracepoints all from a Go API. The tradeoff with using
\texttt{ebpf-go}/Go instead of C is that we can develop the frontend in an
easier language to work with at the cost of having access to a smaller, less
mature ecosystem and toolchain. However, this is not a problem for \afss: the
design phase did not specify any features which are not accessible using the
\texttt{ebpf-go} toolchain. 

We used an x86 server running Ubuntu 24.04.2 with Linux
kernel 6.8.52-generic for development. We used the \texttt{ebpf-go} toolchain to compile the
\ac{bpf} code at version 0.17.1 and used Go version 1.23.2. \texttt{ebpf-go} makes use of
Clang version 18.1.3.
Table~\ref{tab:specs_tooling} summarises this information.

\begin{table}[h]
\centering
\caption{Summary of System Specifications and Tooling for \af Implementation}
\label{tab:specs_tooling}
\begin{tabular}{ll}
\toprule % Use \hline if not using booktabs
\textbf{Component} & \textbf{Specification / Version} \\
\midrule % Use \hline if not using booktabs
Operating System      & Ubuntu 24.04.2 \\
Kernel                & Linux 6.8.52-generic \\
Architecture          & x86 Server \\
eBPF Library          & ebpf-go \\
ebpf-go Version       & 0.17.1 \\
Frontend Language     & Go \\
Go Version            & 1.23.2 \\
eBPF Compiler (used by toolchain) & Clang \\
Clang Version         & 18.1.3 \\
eBPF Code Generator   & go2bpf (part of ebpf-go) \\
\bottomrule % Use \hline if not using booktabs
\end{tabular}
\end{table}

\subsection{eBPF Helper Functions}

Including the \texttt{bpf/bpf\_helpers} header file gives you access to a wide
variety of \ac{bpf} \textit{helper functions}: functions which are written in
the kernel instead of \ac{bpf} and provide common functionality such as reading
data from structs \cite{ebpf-helper-functions-2025, ebpf-bpf-probe-read-user-2024}. 
These are needed as lots of functions are unrepresentable in 
\ac{bpf} due to the verifier's strictness - in fact, \ac{bpf} is not Turing
complete \cite{gregg-bpf-performance-tools-2019}. Kernel functions often have
corresponding \ac{bpf} helpers, which we make use of in Sections
\ref{subsec:impl-syscall-site} and \ref{subsec:impl-find-vma}.

\subsection{Core Filtering Logic}

\afg core filtering logic is the novel part of the implementation, and so is
discussed here in detail. This section aims to present \textit{how} we used
specific kernel data structures and \ac{bpf} helper functions to implement the
steps we outlined in Section \ref{sec:design}, as well as highlight
implementation challenges.

\subsubsection{Executing on Every system call}

The Design section specifies that the core filtering logic needs to be run on
every system call. This was a challenging problem to solve, as most \ac{bpf}
use cases are targeted to specific actions on specific events and not a broad
class of events. Furthermore, a lack of documentation meant that many hours were
spent researching by reading source code and commit messages in order to get to
a working implementation.

To do this, we \textit{attach} our \ac{bpf} filtering program to
the \texttt{raw\_tp/sys\_enter} tracepoint using the \texttt{SEC} macro provided
by the \texttt{bpf/bpf\_helpers} header file, visualised in Figure~\ref{fig:syscall_rawtp_flow}.

A tracepoint is a static instrumentation marker inserted into code that lets tracing 
tools collect runtime diagnostics or performance data without altering program 
behaviour. \ac{bpf} programs can be \textit{attached} to tracepoints, meaning they 
are run when some code path passes the tracepoint. Linux machines have many 
different tracepoints for different events, including one for each different type of 
system call. 

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[
        % Node styles
        block/.style={rectangle, draw, text centered, rounded corners, minimum height=1.8em, text width=5cm, inner sep=4pt},
        highlight/.style={rectangle, draw=red!70!black, fill=red!10, text centered, rounded corners, minimum height=1.8em, text width=5cm, inner sep=4pt, thick}, % Style for the raw_tp node
        % Arrow style
        line/.style={draw, thick, -{Stealth}},
        % Positioning
        node distance=0.8cm % Vertical space between nodes
    ]

    % Nodes
    % Corrected text inside the first node label - removed the extra '}'
    \node (user_start) [block] {User Space\\(\small Initiates system call)}; % <-- Corrected text inside {} and ensured ending ';'
    \node (raw_tp) [highlight, below=of user_start] {{\textbf{raw\_tp/sys\_enter}\\(\small BPF Filter Hook Here!})};
    \node (kernel_handler) [block, below=of raw_tp] {{Kernel system call Handler\\(\small Processes Request})};
    \node (user_end) [block, below=of kernel_handler] {{User Space\\(\small Receives Result})};

    % Arrows
    \path [line] (user_start) -- node [right, pos=0.5] {\small Enters Kernel} (raw_tp);
    \path [line] (raw_tp) -- node [right, pos=0.5] {\small Continues to Handler} (kernel_handler);
    \path [line] (kernel_handler) -- node [right, pos=0.5] {\small Returns Result} (user_end);

    \end{tikzpicture}
    \caption{Simplified system call Execution Flow with raw\_tp/sys\_enter Tracepoint}
    \label{fig:syscall_rawtp_flow}
\end{figure}

These system call-specific tracepoints make the syscall's arguments available to the 
\ac{bpf} program attached to the tracepoint in a type-safe way. However, these user-
friendly system call-specific tracepoints do not fulfil the design requirements for \af 
as they will not execute for every system call. Therefore, we elected to use a
raw tracepoint.

The \texttt{sys\_enter} raw tracepoint is executed on every system call before the
kernel starts to process the system call. This is helpful, as it allows us to block
a system call (if need be) before the kernel begins to execute it. We also need
access to the system call number, a syscall argument on Linux/x86. Unhelpfully, 
raw tracepoints do not make the system call arguments readily available to the attached 
\ac{bpf} program. Instead, the attached program receives a pointer to an array of 
zero length (a C pattern for defining a ``variable length'' array) with arguments 
differing per-system call. One commonality between every syscall is that the syscall 
number is available as the second argument in the array. 

\begin{table}[h!]
\centering
\caption{Comparison of \texttt{raw\_tp/sys\_enter} and system call-Specific Tracepoints for Filtering Every Syscall}
\begin{tabular}{|p{6cm}|p{6cm}|}
\hline
\texttt{raw\_tp/sys\_enter} & \textbf{system call-Specific} \\
\hline
\multicolumn{2}{|c|}{\textbf{Pros}} \\
\hline
Executes on \textbf{every} system call & Type-safe access to arguments \\
Single attachment for all system calls & Arguments easily available/structured \\
Runs before kernel processes system call & Easier to inspect args for \textit{that} syscall \\
system call number available (arg 1) &  \\
\hline
\multicolumn{2}{|c|}{\textbf{Cons}} \\
\hline
Generic argument access (pointer to array) & Only triggered by a \textbf{specific} system call \\
Arguments less easily available & Requires many attachments for all system calls \\
Less user-friendly interface for arguments &  \\
\hline
\end{tabular}
\label{tab:tracepoint_comparison}
\end{table}

Therefore, attaching the core \ac{bpf} program to a \texttt{sys\_enter} raw
tracepoint allows \af to run on every system call and provides access to the
system call number, making the tracepoint a suitable attachment site for our
program. The pros and cons of using a \texttt{raw\_tp} are summarised in
Table~\ref{tab:tracepoint_comparison}. Attaching the \ac{bpf} program is done by calling a Go function that
\texttt{ebpf-go} generates from the \ac{bpf} source. 

Having established how and when the \af filtering logic will run, we can now
turn our attention to the business logic.

\subsubsection{Fork Following}

The filtering logic must first decide whether we
need to apply our filter to the current system call. It must also implement fork following. 

Fork following is the idea that we should apply our filter to any children of a
filtered process. If we apply a filter to an app and it forks/execs,
the child process should inherit the same filter. This idea was
presented in Section \ref{subsec:design-fork-following} and relies
on a ``follow'' map and the \ac{pid}/parent \ac{pid} of the calling
process.

We created a ``follow'' map using a \texttt{BPF\_MAP\_TYPE\_HASH} which mapped
\ac{pid}s to booleans, effectively implementing a set data type. (P)\acp{pid}
are read from the current process's \texttt{task} struct (the kernel data
structure containing process information) obtained by calling the
\texttt{bpf\_get\_current\_task\_btf()} helper function. 

On each system call, we check to see if the calling \ac{pid} is present in the
``follow'' map using the \ac{bpf} helper function \texttt{bpf\_map\_lookup\_elem()}.
If it is, we continue the program. If not, we check whether
the parent \ac{pid} is in the map. If neither ID is present,
we return from the program with an error code of 0. Otherwise, we
use \texttt{bpf\_map\_insert\_elem()} to add the calling \ac{pid} to
the  ``follow'' map.

\subsubsection{Finding the system call Site}\label{subsec:impl-syscall-site}

Next, \af needs to find the system call site - the \textbf{first non-\ac{libc} return pointer} in the userspace stack when the syscall was made.

To do this, we pulled the first 16 stack frames from the user stack with the 
\texttt{bpf\_get\_stack()} helper function and the \texttt{BPF\_F\_USER\_STACK}
flag. \texttt{bpf\_get\_stack()} returns an array of return pointers, with the
topmost stack frame at index 0. 

To identify the first non-\ac{libc} return pointer, we started iterating over
the list of return pointers and finding which library each belonged to.
By using the \texttt{bpf\_find\_vma()} helper to map each return pointer to its
corresponding shared library, we were able to find the addresses which belonged
to \ac{libc} and skip over them. Then, we broke out of the loop when we found a
filename with \texttt{bpf\_find\_vma()} that was not \ac{libc}, therefore
identifying the first non-\ac{libc} return pointer.

This approach was accurate but too slow to be performant. Calling \texttt{bpf\_find\_vma()} is
slow, as it walks the in-kernel \ac{vma} red-black tree. Red-black trees scale
well but are slow to use as they are a pointer-based data structure \cite{pointer-based-ds-slow}. Moving to
a child node involved loading memory that is likely not already in the cache.
Finding one system call site often involves walking through multiple \ac{libc}
addresses before finding the first non-\ac{libc} return pointer. When run in
verbose mode, \af collects runtime statistics
(Section~\ref{subsubsec:err-handling}): informally inspecting the logs shows that
applications like Redis have anywhere from 3-7 stack frames with return pointers 
within \ac{libc}. This will result in 4-8 lookups per system call (including the
first non-\ac{libc} return pointer), detrimentally affecting performance.

\subsubsection{Optimising system call Site
Finding}\label{subsubsec:impl-find-site-opt}

We noted that many system call sites are frequently reused, and could
therefore maintain a mapping from the return pointer to the relevant whitelist. However,
this approach suffers from issues with cache coherency. While effective at
reducing lookups, the cache would need to be flushed every time a system call which
can change the \ac{vma} is made (e.g. \texttt{mmap}, \texttt{munmap},
\texttt{brk}, etc\dots). 

This would require writing another \ac{bpf} program to
monitor these calls and invalidate stale address ranges. This would need significant
engineering effort orthogonal to our research. We would also need to identify
which tracepoint(s) (or instrumentation points) this program must be
attached to. We would have to configure this program to run \textbf{before} the
\af core program, which would be a challenge as the first instrumentation point
in the system call path is the \texttt{sys\_enter} raw tracepoint.

Having considered these alternatives, we decided on a much simpler
implementation. Our solution to speed up identifying whether return pointers
belong to \ac{libc} is to cache \acg{libc} start and end pointers in \iac{bpf}
map which we call the \texttt{libc\_ranges} map. In 
Section~\ref{subsec:objs-assumpions-tm} we assume that the  \ac{libc} address space 
will be contiguous and will not change during program execution. Therefore, we
do not need to build logic for cache invalidation.

The design section also required that we
would not introduce a security risk if these assumptions were invalidated. Using a cached \texttt{libc\_ranges} map
respects this design criterion. If an attacker moves the address of \ac{libc},
the stack pointer will be identified as the first non-\ac{libc} return
pointer. Therefore, \af will try to use a \ac{libc} whitelist to determine
whether to allow the system call. Since \ac{libc} whitelists will not exist due to
the way whitelists are generated
(Section~\ref{subsubsec:impl-whitelist-gen}), this will \textbf{always result in
\af intervening}.

This optimisation resulted in a \textbf{43\% increase in request throughput} in the Redis
benchmark. We used the same benchmark options and test bench as outlined in
Section~\ref{sec:evaluation} and details are given there. Table~\ref{tab:throughput} shows results from the
optimisation.

\begin{table}[ht]
  \centering
  \begin{tabular}{l r}
    \toprule
    \af version                       & Request per second (3sf) \\
    \midrule
    Baseline (no \afss)           & 126,000                 \\
    Pre-optimisation               & 73,200                  \\
    Post-optimisation              & 104,000                 \\
    \midrule
    Total \% increase in throughput & 43\%                    \\
    \bottomrule
  \end{tabular}
  \caption{Throughput comparison for Redis before and after optimising the stack 
  walking functionality}
  \label{tab:throughput}
\end{table}

In the case where there was no non-\ac{libc} address in the first 16 stack
frames, we make subsequent calls to \texttt{bpf\_get\_stack()} to get the next
16 frames until either we find a non-\ac{libc} address or we see a 0, in which
case the stack has ended. If this happens, we mark that there was no
non-\ac{libc} address present in the ``stats'' map
(Section~\ref{subsubsec:err-handling}).

With system call-site discovery now optimised, the next task is to associate each 
identified the call site with the shared library that contains it.

\subsubsection{Finding the Shared Library File}\label{subsec:impl-find-vma}

Finding the shared library file, which contains the address previously marked as
the system call site, is done by traversing the \ac{vma} red-black tree in the
kernel. This is done using the \texttt{bpf\_find\_vma()} helper, which was used
in the unoptimised implementation of the system call site identification logic.

\texttt{bpf\_find\_vma()} returns the \texttt{vm\_area\_struct} struct associated with the
return pointer's address. The \texttt{vm\_area\_struct} struct is a kernel data structure
which holds information about the process's \ac{vma}, including addresses,
permissions, and the file which backs the memory. Here, we are interested in the
file which backs the memory region. We read this by supplying a function
to the \texttt{bpf\_find\_vma()} helper, which is called when the right
\texttt{vm\_area\_struct} struct is found. Then, we use \texttt{bpf\_probe\_read()} to make
the series of reads needed to find the filename.

We then use the filename as a key in another \ac{bpf} map - the ``whitelists''
map. This is a \texttt{BPF\_MAP\_TYPE\_HASH} which associates strings (filenames
of shared libraries) to whitelists. This map is populated by the Go frontend
with pre-generated whitelists before the \ac{bpf} program is attached to the 
\texttt{raw\_tp/sys\_enter} tracepoint and is discussed in 
Section~\ref{subsec:impl-frontend}.

\subsubsection{Checking the Whitelist}

Whitelists are implemented as a bit array, 56 bytes long. Bit position $i$ is
set to $1$ if and only if system call number $i$ is allowed. We opted for a bit
array as it is a compact, efficient data structure and can be queried easily.

To check if a system call is allowed, we divide the syscall number by 8 to identify  
which byte the relevant bit is in. We then take the system call number modulo 8 to
identify which bit in the byte we must look at. If the value of the bit
corresponding to the system call number is $1$, then the syscall is allowed, and we
return with exit code 0. Otherwise, we intervene.

\subsubsection{Intervening on Disallowed System Calls}\label{subsubsec:impl-intervening}

The user can choose between three intervention policies for when a
non-whitelisted system call is made, as specified in 
Section~\ref{subsec:objs-assumpions-tm}. These are to warn the user, kill the
malicious process, or kill all processes in the ``follow'' map.

To support these configurable modes, we used a \texttt{BPF\_MAP\_TYPE\_ARRAY}
called ``cfg\_map''. It is an array of length one which is set by the frontend
before the program is attached. The array stores an integer corresponding to
a ``config-type'' enum and instructs the \ac{bpf} program how to respond to a
disallowed system call.

In kill mode, we only kill the malicious process. This can be done synchronously
(i.e. \textit{before} the kernel begins to process the system call) from \ac{bpf}
using the \texttt{bpf\_send\_signal()} helper function. 
\texttt{bpf\_send\_signal()} can only be used to send signals to the calling
process - by sending a \texttt{SIGKILL}, we kill the malicious process.

If the user has configured warn mode or ``kill all'' mode, then userspace needs
to be involved to some degree. To do this, we use \iac{bpf} ringbuffer to inform
userspace when a system call has tripped the filter. The ringbuffer is a thread-safe
\ac{bpf} data structure, which efficiently transfers data between the kernel and
userspace. When a disallowed system call happens, a struct containing syscall
number and offending \ac{pid} is written to the ringbuffer and quickly read by
userspace. Figure~\ref{fig:intervention-flowchart} visualises this procedure.

\begin{figure}[h]
\centering
\includegraphics[width=0.8 \linewidth]{./diagrams/profiling-flow.drawio.pdf} 
\caption{Flowchart illustrating the intervention strategies based on the
configured mode when a non-whitelisted system call occurs. Actions are separated
into BPF (Kernel) and Frontend (User Space) lanes.}
\label{fig:intervention-flowchart}
\end{figure}

The \ac{bpf} program will return with exit code 0 in warn mode. In ``kill
all'' mode, \af uses \texttt{BPF\_SEND\_SIGNAL()} to kill the current process
before exiting with code 0.

The userspace code will act differently upon receiving a packet in the
ringbuffer. If warn mode is enabled, it will just log the fact that a \ac{pid}
tried to make a disallowed system call. In kill-all mode, it will iterate through
every \ac{pid} in the ``follow'' map and send a \texttt{SIGKILL} to each
process.

\subsubsection{Error Handling and Recovery}\label{subsubsec:err-handling}

In the best case scenario, \af will behave as described above. However, errors
during execution are inevitable and are handled robustly. 

Map lookups often return a pointer to some data. If the lookup fails, the
pointer will be null. Every single pointer is checked (enforced by the \ac{bpf}
verifier), and if any are null, a \textbf{stat is logged} before the program
exits.

These stats are written to a ``stats'' map, which counts the occurrence of
different events. Unlike ringbuffers, concurrent access to maps might cause race
conditions, so the \texttt{\_\_sync\_fetch\_and\_update()} function was used to
increment stat counts atomically. When run in verbose mode, the contents of the
stats map will be read and dumped to \ac{stdout} by the frontend.

The stats map is implemented as a \texttt{BPF\_MAP\_TYPE\_ARRAY} and is indexed
by the \texttt{STAT\_TYPE} enum. A helper function, \texttt{log\_stat()}, was
written, which took an enum option, $i$, as an argument and incremented the value
of the stats map array at index $i$ by one.

While simple, this allows for fine-grained visibility into the \ac{bpf} program.
A stat is recorded at every point the program could exit - a list of recorded
stats are included in Listing~\ref{lst:stat-enum}. Using an array over a
ringbuffer was done to reduce code complexity in the frontend (as no goroutine
reading the ringbuffer is needed), and we do not need the real-time capabilities
of a ringbuffer.

\subsection{Debug Features}

To aid observability when debugging, we also implemented some extra features
on top of the aforementioned \textit{verbose} mode.

\subsubsection{Logging}\todo{cut if need}

A \textit{DEBUG} mode,
which prints execution info \textbf{from \ac{bpf}} to the
\texttt{/sys/kernel/tracing/trace\_pipe} pseudofile, was implemented for
kernel-side logging. This incurs a substantial performance penalty, so it is only
activated when the \ac{bpf} source is compiled with the \texttt{\#DEBUG} macro
defined.

\subsubsection{Profiling}

We also instrumented \af to report profiling information for each stage of the
filtering process. A profile for a ``Hello, World'' application is provided in 
Appendix~\ref{tab:profile-info}. Profiling information was used to identify that
finding the system call site was the slowest step of \af before the optimisation
discussed in Section~\ref{subsubsec:impl-find-site-opt} was applied.

Gathering profile data was implemented with a ringbuffer despite the extra
implementation complexity over an array with atomic read/writes. This design was
needed to guarantee that every call record would be consumed by the frontend since a 
simple array risked having entries overwritten before they could be processed.
If the ringbuffer is full, then a stat is recorded in the stats map, and no
profiling information is recorded for that execution.

Timestamps were recorded after every stage of the filtering using the
\texttt{bpf\_ktime\_get\_ns()} helper, which returned the \textbf{kernel time}:
the number of nanoseconds elapsed since boot. This is a monotonically increasing
counter which is not affected by leap seconds and other timing oddities \cite{ebpf-bpf-ktime-get-ns-2025}. In the
case that a failure occurs during one of the stages of \afss,~the profiling data
is discarded. This process is visualised in Figure~\ref{fig:profiling-data-flow}


\begin{figure}[hbpt]
\centering
\includegraphics[width=\textwidth]{./diagrams/profiling-flow-actual.drawio.pdf}
\caption{A flowchart showing how profiling data is written to and parsed by
userspace.}
\label{fig:profiling-data-flow}
\end{figure}

Writing to a ringbuffer requires locking and is, therefore, an expensive
operation. Therefore, we only included the code needed to profile the
application when the \texttt{\#PROFILE} macro was defined at compile time.
We chose to use a regular ringbuffer over a 
\texttt{BPF\_MAP\_TYPE\_PERF\_EVENT\_ARRAY} as we did not need all of the
information that \texttt{perf} provides and would have made data processing
unnecessarily complex.

In the frontend, a goroutine dedicated to reading the ringbuffer was spawned.
Upon reading data, it computes the difference between the timestamps for each 
packet in the ringbuffer and writes this information to a buffered \ac{io} 
writer (\texttt{bufio}) in Go. This writer is flushed when the profiling 
ringbuffer is closed: either if the user presses Ctrl-C, an error during 
reading occurs, or the program being profiled finishes/trips the system call 
filter.

\subsection{Configuration and the Go Frontend}\label{subsec:impl-frontend}

Accounting for roughly 60\% of the project's 5,100 \ac{loc}, the frontend handles 
everything from the \ac{cli} and BPF  map/data structure initialisation, to 
launching the program, capturing live data via ringbuffers, and inspecting 
map contents after execution. A flow chart showing an overview of the frontend's
functionality is provided in Figure~\ref{fig:frontend-flowchart}.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{./diagrams/frontend-flow.drawio.pdf}
\caption{Overview Flowchart of the Go Frontend Execution Logic.}
\label{fig:frontend-flowchart}
\end{figure}

The \ac{cli} is built using the \texttt{urfave/cli/v2} Go framework. It provides
the main command to filter an application, ``\af'', which also takes a whitelist
(generated by \texttt{afgen /path/to/executable args/...}) and an
executable and its arguments. 

The user is able to configure the application to warn when a disallowed system call
is made by using the \texttt{--warn} flag, or to kill all processes with the
\texttt{--kill-all} flag. 

We provided \texttt{--verbose}, \texttt{--profile}, and \texttt{--spawn-root}
options to let the user see a stats dump or record profile information. The
\texttt{--spawn-root} option will allow the user to spawn the filtered
application as root. This is intended purely for development as if exploited,
an application with root privileges would just be able to detach \af from its
tracepoint, rendering it useless.


\subsubsection{Handling different configuration options}

The configuration mode that the user has selected is written to the
``cfg\_map'' (referenced in Section~\ref{subsubsec:impl-intervening}) and also
recorded in a \texttt{FilterCfg} struct by the frontend. When ``kill-all'' or
``warn'' mode is selected, we spawn a dedicated goroutine to listen for packets
being written to the ringbuffer. In ``warn'' mode, any packets read will be
logged using a \texttt{SugaredLogger} from the \texttt{uber/zap} logging
library. 

In ``kill all'' mode, reading a packet from a ringbuffer will trigger
the frontend to kill all the processes in the follow map. It does this by
iterating over the \acp{pid} in the follow map and sending a \texttt{SIGKILL} to
each process using the Go standard libraries \texttt{os.FindProcess()} and
\texttt{Process.Kill()} functions.

Before spawning the supplied executable, the frontend needs to initialise all
the maps that \af will need during execution. This involves parsing the
supplied whitelists and loading them into the ``whitelists'' map. 

We also need to populate the \texttt{libc\_ranges} map introduced in
Section~\ref{subsubsec:impl-find-site-opt} with the filtered processes \ac{libc}
start and end addresses. Due to \ac{aslr}, this must be done once the process
has already been spawned. 

We also need to add the spawned process's \ac{pid} to the ``follow'' map to
activate the filter. Again, this needs to be done after the process has been
spawned, as before then, we do not know its \ac{pid}. This design's key limitation: \textbf{there will be a period of time during program startup where \af 
does not protect the filtered application}.

\subsubsection{Spawning the executable} \label{subsubsec:spawning-exec}

To spawn the executable, we use the \texttt{os/exec} package from the Go
standard library to create a \texttt{Command} with the executable and arguments
passed in through the \ac{cli}. This was done with the \texttt{CommandContext()}
function, which also returns a \texttt{context.Context} that kills the
application when the context is cancelled. This ensured that we retain control
over the lifetime of the spawned application and that it will be killed if \af
errors.

Care needs to be taken when configuring the command to ensure we do not 
introduce a security flaw. A user needs the \texttt{CAP\_SYS\_ADMIN} privilege
to attach \iac{bpf} program, and therefore \af needs to be run by a user with
the \texttt{CAP\_SYS\_ADMIN} privilege. However, since we are spawning an
executable via an \texttt{execve} system call (\texttt{Command.Start} uses
\texttt{execve} behind the scenes), the filtered process will also inherit the
\texttt{CAP\_SYS\_ADMIN} privileges.

Our threat model (Section \ref{subsec:objs-assumpions-tm}) assumed that an attacker
had compromised the filtered application. If the filtered application had
\texttt{CAP\_SYS\_ADMIN} privileges, then an attacker might be able to change
information in the \ac{bpf} maps that \af relies on and evade filtering. If an
attacker could remove all \acp{pid} from the follow map, they would render
\af completely useless. 

Therefore, we took care to \textbf{change the UID of the process being spawned}
to an arbitrary number (we chose 1000). This has the effect of
\textbf{de-privileging the filtered application} and ensuring that no adversary
can alter key \ac{bpf} data structures.

% 1. User launching a process filtered by addrfilter needs CAP_SYS_ADMIN
% privileges
% 2. On an execve, the child process inherits the UID from the parent
%  => inherits all privileges from the parent.
% 3. By default, this means that the filtered application will have
% CAP\_SYS\_ADMIN privileges, and therefore, an attacker could detach the filter

\subsubsection{Configuring the \texttt{libc} map}
Once the process is spawned, the frontend can access its \ac{pid} stored in the
\texttt{Command.Process} struct. We implemented a function, 
\texttt{findLibc()}, which accepts a \iac{pid} as an argument and returns a pointer to 
a \texttt{LibcRange}  struct. The \texttt{LibcRange} struct holds the start and end 
address of \ac{libc}.

\texttt{findLibc()} finds the start and end address of \ac{libc} by parsing the
\texttt{/proc/PID/maps} pseudofile in userspace. Each line of \texttt{/proc/PID/maps}
contains information about \texttt{vm\_area\_struct} struct in the process's \ac{vma}. We
built a regular expression to match each line of \texttt{/proc/PID/maps} and
used match groups to extract the \texttt{vm\_area\_struct} struct's start and end address.
We then tracked the smallest and largest start/end addresses we saw 
and set these to be \acg{libc} start and end addresses.

We found in development that we would sometimes begin parsing the pseudofile
before the linker had loaded each \texttt{vm\_area\_struct} struct. This led to
\af exiting reporting that \textit{no \ac{libc} addresses were found}, and in
the worst case, confusing bugs where system call sites were incorrectly being marked as 
having come from \ac{libc}. This happened as parsing started while the
linker was loading \ac{libc}, so some \ac{libc} addresses were not being skipped
when finding the system call site.

The fix here was to add retry logic to the parsing. If no \ac{libc} was found,
we increment a retry counter, close the file, and retry parsing. Each time
we parse, we count the number of lines in the pseudofile: if \ac{libc} is found,
we close and reopen the file and check its line count. If they are the
same, we know we have accounted for every \texttt{vm\_area\_struct} struct 
mapped to \ac{libc}.


\subsubsection{Exiting Gracefully}

As mentioned in Sections \ref{subsection:tech-stack} and 
\ref{subsubsec:spawning-exec}, we use Go's \textit{Context} concurrency pattern
to handle graceful exits. 

Each long-lived function\footnote{Examples of long-lived functions are functions
that read from ringbuffers in an infinite loop and child processes spawned using
\texttt{os/exec}.} is passed an objecting implementing the 
\texttt{context.Context} interface as a parameter. These functions all wait to
receive on the \texttt{Context.Done()} channel: when they receive word that the
context is done, we configured the functions to clean up resources and exit. 

When the filtered application exits, receives a \texttt{SIGKILL} or 
\texttt{SIGINT}, or \af encounters an error it cannot recover from, we 
\textit{cancel the context}. This means that each long-lived function receives 
a signal on the \texttt{Context.Done()} channel, and therefore all goroutines 
exit, and all resources are closed.

\ac{bpf} maps are cleared by default when the process which mounted them exits,
as we did not \textit{pin the maps} (make the maps persist). Similarly, the \af
program will be automatically removed from the \texttt{raw\_tp/sys\_enter}
tracepoint.

When running in \texttt{--verbose} mode, the frontend will read the contents of
the stats map before exiting and dump its contents to \ac{stdout} as JSON.

% Need PID to start filtering: ~1 second where filter is inactive
% Requires dynamically linked binaries
% Unwinding the stack is brittle (rely on FPs; other trampolines (ftrace) can 
% break the process; bpf_get_stack() a bit cursed.
% bpf tracing interference with \af enabled: trampolines used by kfuncs can
% interfere with how the userspace stack is represented, with 0s (normally
% signalling the end of the stack) being placed in the middle of the stack TODO:
% cite that lvn thing I sent to father.

\subsection{Additional Tooling}

Alongside \afss,~we also implemented additional tooling. This tooling is either
orthogonal to our research (\texttt{afgen}) or development tooling
meant to aid evaluation (\texttt{af-seccomp}, \texttt{syso}), and so will not be
discussed in detail. 

\subsubsection{Whitelist Generation}\label{subsubsec:impl-whitelist-gen}

Generating whitelists for \af involved a lot of the same steps as the filtering
program, so it did not require much engineering effort. To be able to reuse code,
we structured our code base carefully: \ac{bpf} maps and common data structures
were kept in the \texttt{bpf/} directory, with the filtering program and
whitelist generation program kept in \texttt{bpf/filter/} and
\texttt{bpf/wlgen/} directories respectively. This exercise was complicated by
the fact that the \ac{bpf} toolchain does not provide a linker, so workarounds
included \texttt{\#include}-ing \ac{bpf} source files.

The \ac{bpf} program reused the fork following system call site identification and
filename finding functionality from \afss. Instead of checking to see if a
system call is on in the whitelist, we add the syscall number. 

The frontend for the generation functionality spawns the application similarly as \af and allows for the same debug/profiling options. Instead of parsing a
whitelist and writing this to the whitelists map, the map starts empty. Then,
when the application exits, the whitelist map is read, and whitelists are
marshalled.

The whitelist generation suffers the same blind spot as \af in its early stages
of execution due to the need to find the \ac{libc} address before being able to
start filtering. This did not prove problematic during the Evaluation phase, so
it is an acceptable solution to whitelist generation for the purposes of this
report. Note that in production, dynamic-analysis based generation is \textbf{a
bad idea}, as it can easily lead to false positives, which will unnecessarily
kill an application.

\subsubsection{Seccomp Filtering for Evaluation}\label{subsubsec:syso-impl}

To be able to compare the runtime cost of \af to the current gold standard
filtering solution, we implemented a program to parse our whitelist format,
flatten the whitelists, and use the list to create a seccomp filter. The program
then launched an application without the added complexity surrounding changing
a UID.

To create and attach the seccomp filter, we used a Go library 
\texttt{libseccomp-golang} at version 0.10.0. \texttt{libseccomp-golang}
provides a Go \ac{api} to seccomp, meaning that filters can be defined from a
Go application. This allowed us to reuse our whitelist parsing and application
spawning code.

We started by creating a seccomp filter with \texttt{libseccomp.NewFilter()}, 
and set its default action to \texttt{libseccomp.KillProcess}. We then parsed
and flattened the (\afss)~ whitelist passed to \texttt{af-seccomp} via the
\ac{cli}, and added an \texttt{libseccomp.Allow} rule for each system call number in
the whitelist.

\subsubsection{Calculating Privilege Reduction}

To calculate the privilege reduction gained by using \af, we implemented a
tool called \texttt{syso}. It is an evaluation tool that uses dynamic analysis
to track which shared libraries make which system calls. It does
stack-unwinding in the kernel and aligns the addresses with values in
\texttt{/proc/PID/maps}. 

When execution finishes, \texttt{syso} writes a report to \ac{stdout} with three
figures: the privilege score of the application filtered with seccomp, the
privilege score of the application filtered with \afss,~ and the percentage
privilege reduction when \af is used instead of seccomp. The raw data is stored
as JSON in \texttt{./stats/counts.json} and a dump of runtime statistics is
stored in \texttt{./stats/missed.json}.

Having gone over the implementation in detail, we may move on to evaluating
\af's performance. In the next section, we show that \af can provide a
significant level of privilege reduction for an added, but not detrimental,
runtime cost.

