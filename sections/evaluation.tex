\section{Evaluation}\label{sec:evaluation}

\iffalse
    Gavin wants to see:
    - What is the max privilege score that an application can have?
    - Diagram about system call distribution
\fi

Our evaluation demonstrates that \af significantly reduces application
privilege compared to seccomp, quantifies the associated runtime overhead, and
validates \af's correctness. This section details the test environment, the
benchmark suite used, the validation process, the security benefits measured
via a novel \textit{probabilistic} privilege metric, and the performance impact 
relative to seccomp and an unfiltered baseline.  

\subsection{Test Environment}

Our test environment was the same as our development environment outlined in
Section~\ref{subsection:tech-stack} and is presented in
Table~\ref{tab:test_environment}.

The kernel used default settings - \ac{aslr}, stack
canaries, and other default security features enabled for all tests.
\af was run as root.

\af (v1.0.0, commit
\href{https://github.com/tcassar-diss/addrfilter/tree/2bd209c630df3509d4ac721d018dabab94305dde}{\texttt{2db209c}},
available at
\href{https://github.com/tcassar-diss/addrfilter/releases/tag/v1.0}{github.com
/tcassar-diss/addrfilter}) was compiled using Go v1.23.2. The \ac{bpf} components
were compiled using the Cilium \texttt{ebpf-go} toolchain (v0.17.1) with
Clang v18.1.3 as the LLVM backend. No non-default compiler flags were used.

\begin{table}[h]
    \centering 
    \caption{Test Environment Configuration Summary}
    \label{tab:test_environment}
    \begin{tabular}{@{}ll@{}} 
        \toprule
        Component           & Specification \\
        \midrule
        \multicolumn{2}{@{}l}{\textbf{System Hardware \& OS}} \\ 
        \aclp{cpu}           & 2x Intel Xeon Gold 5220R \\
        Operating System    & Ubuntu 24.04.2 \\
        Kernel              & Linux 6.8.52-generic \\
        Logical Cores       & 96 (2 sockets x 24 cores/socket x 2 threads/core) \\
        System Memory       & 64 GB @ 2666 MT/s \\
        Storage             & 2 TB (Buffered Read: \textasciitilde{}515 MB/s via \texttt{hdparm}) \\
        Kernel Security     & Standard defaults enabled (ASLR, Stack Canaries, etc.) \\
        \midrule
        \multicolumn{2}{@{}l}{\textbf{Software \& Build Environment}} \\ 
        Software Under Test & \texttt{addrfilter} v1.0.0 (Commit: \texttt{2db209c}) \\
        Go Compiler         & Version 1.23.2 \\
        eBPF Toolchain      & Cilium ebpf-go v0.17.1 \\
        LLVM Backend (for eBPF) & Clang version 18.1.3 \\
        \midrule
        \multicolumn{2}{@{}l}{\textbf{Benchmarking Configuration}} \\ 
        Network Tests       & Client and Server processes co-located on host \\
        Application Privileges & Filtered applications run as non-root \\
        \bottomrule
    \end{tabular}
\end{table}


For network benchmarks (\texttt{redis-benchmark}, \texttt{wrk}), client and
server processes ran on the same host to isolate performance overhead from
network latency. 

\subsection{Benchmark Selection}\label{subsec:benchmark-selection}

We evaluated \af's security and performance using a benchmark suite (versions
in Table~\ref{tab:benchmark_software_versions}) selected to cover diverse
server workloads and stress different system call patterns: network-bound (Redis,
Nginx), \ac{io}-intensive (\texttt{fio}), \ac{cpu}-intensive (\ac{npb}),
and mixed (PostgreSQL). Several of these benchmarks have been used in
literature to evaluate tools like gVisor \textcite{lerner2019gvisor}.

\begin{table}[h] 
    \centering 
    \caption{Benchmark Suite Software Components and Versions}
    \label{tab:benchmark_software_versions}
    \begin{tabular}{@{}ll@{}} % Left-aligned columns, @{} removes side padding
        \toprule
        Software Component    & Version \\
        \midrule
        Redis Server          & 7.0.15 \\
        \texttt{redis-benchmark} & 7.0.15 \\
        Nginx                 & 1.24.0 \\
        \texttt{wrk} (Load Generator) & 4.1.0 \\ % Added context for wrk
        \texttt{fio} (I/O Tool) & 3.39-38-gf18c \\ % Added context for fio
        PostgreSQL            & 16.8 \\
        NPB Suite             & 3.4.3 \\ % Assuming this is the NAS Parallel Benchmarks suite based on common usage
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Redis and \texttt{redis-benchmark}}\label{subsubsec:redis-just}

Redis, an in-memory key/value store, represents latency-sensitive, network-bound
applications. Its workload primarily stresses network (\texttt{accept()},
\texttt{read()}, \texttt{write()}, \texttt{epoll\_wait}) and memory management system calls, allowing measurement of \af's impact on high-throughput, low-latency services.

Redis ran with default settings. \texttt{redis-benchmark} also used defaults (3-byte keys, increasing relative system call intensity \cite{zhou2023userspace}) but simulated 48 clients (\texttt{-c 48}, using half the server's cores) to introduce moderate, realistic concurrency.

\subsubsection{Nginx and \texttt{wrk}}\label{subsubsec:nginx-just}

Nginx, a high-performance web server, represents applications managing concurrent network connections and file \ac{io}. This benchmark exercises network, file system (\texttt{open}, \texttt{read}, \texttt{sendfile}), and process/thread management system calls, testing \af under high concurrency and mixed \ac{io}.

Nginx configuration (Listing~\ref{lst:nginx-conf}, Appendix~\ref{subsec:nginx})
deviated from defaults: \texttt{worker\_processes} was set to auto (uses
available cores), and \textit{sendfile} was enabled for optimised static
file serving (Listing~\ref{lst:index-html}). Access logging was disabled to
minimise disk \ac{io} noise, isolating network and core processing overhead. \
texttt{wrk} generated load using 200 concurrent connections across 4 threads
(\texttt{-c 200 -t 4}) for 30 seconds, stressing connection and process management.

\subsubsection{\texttt{fio}}\label{subsubsec:fio-just}
\texttt{fio}, \iac{io} workload generator, was used to evaluate \af's impact
on the storage subsystem via filesystem and block device system calls. We
focused on high-throughput sequential writes, common in data ingestion tasks
\cite{WRITES_ARE_SEQUENTIAL}.

We configured \texttt{fio} to write 10GB files per job (\texttt{--size=10G})
across 16 parallel jobs (\texttt{--numjobs=16}) for 30 seconds
(\texttt{--runtime=30s}) after a 2-second ramp-up (\texttt{--ramp\_time=2s}) to
ensure stable measurements \cite{fio-docs}. Direct \ac{io} (\texttt{--direct=1})
bypassed the \acg{os} page cache, ensuring write operations directly invoked
\texttt{write()} system calls, increasing sensitivity to \af's overhead. A large
block size (\texttt{--bs=1M}) characteristic of sequential throughput, and high
\ac{io} queue depth per job (\texttt{--iodepth=64},
\texttt{--iodepth\_batch\_submit=64}, \texttt{--iodepth\_batch\_complete\_=64})
maximized load on the storage subsystem. This configuration stresses \af under 
heavy, parallel, direct write load.

\subsubsection{PostgreSQL}\label{subsubsec:psql-just}

PostgreSQL, a complex relational database, represents database workloads. 
\texttt{pgbench} simulates transactions involving client connections 
(network stack), query processing (\ac{cpu}/memory), and write-ahead
logging (sequential, synchronous disk \ac{io} via \texttt{write()}, 
\texttt{fsync()}). PostgreSQL's complexity allows evaluating \af's 
performance across diverse system calls.

The Postgres server ran manually with default settings. \texttt{pgbench} simulated
48 concurrent clients (\texttt{-c 48}) with corresponding worker threads 
(\texttt{-j 48}), using half the server's cores to impose significant load
without saturating resources like the CPU scheduler. Each test ran for 30
seconds (\texttt{-T 30}). 

\subsubsection{The \ac{npb} Suite}\label{subsubsec:npb-just}

The \acl{npb} suite comprises computationally intensive benchmarks derived
from fluid dynamics. This CPU bound workload involves fewer I/O or network
system calls than other benchmarks, isolating \af's overhead on compute-heavy
processes where system calls primarily relate to thread management 
(\texttt{futex()}) and memory management (\texttt{mmap()}). 

We used the standard OpenMP suite configuration (Listings~\ref{lst:npb-make-def},
\ref{lst:npb-suite-def}) with workload size \texttt{C}. This provided
stable readings within a reasonable runtime (less than three minutes per suite run).

\subsection{Validation}\label{subsec:validation}

We validated \af using manual testing and the Linux Test Project (\ac{ltp})
suite \cite{LINUX_TEST_PROJECT}. 

Manual testing during development used minimal applications and custom
shared libraries (Appendix~\ref{sec:linkage}, 
Listings~\ref{lst:getpid-c},~\ref{lst:printf-c}) to confirm that: 

\begin{enumerate}
    \item \af correctly attributed system calls to shared libraries
        (e.g., \texttt{write()} to \texttt{printf.so}, \texttt{getpid()} to 
        \texttt{getpid.so}, per Listing~\ref{lst:whitelist-toml}).
    \item \af terminated processes making system calls not present in their
        respective library's whitelist.
\end{enumerate}

To ensure \af did not adversely affect general system call behavior, we used
the \ac{ltp} system call test suite (\texttt{/opt/ltp/kirk -f
ltp -r syscalls}), installed per the documentation 
\cite{ltpQuickstart}. We compared
results running the suite unfiltered (baseline) against running it under \af
using an \texttt{afgen}-generated whitelist 
(\texttt{afgen ./whitelist.json/opt/ltp/kirk...}). 

Table~\ref{tab:af_failures} shows the 11 additional test failures introduced
when running \ac{ltp} under \af.

\begin{table}[h]
\centering
\caption{\ac{ltp} test failures introduced by \af.}
\label{tab:af_failures}
\begin{tabular}{c@{}}
\toprule
\textbf{Test Failures Introduced with \af} \\
\midrule
 \texttt{process\_vm\_writev02}\\
 \texttt{process\_vm\_readv02} \\
 \texttt{process\_vm\_readv03} \\
 \texttt{perf\_event\_open01} \\
 \texttt{getegid02\_16}        \\
 \texttt{getgroups01}         \\
 \texttt{getegid02}           \\
 \texttt{getgid03}            \\
 \texttt{semctl06}            \\
 \texttt{openat03}            \\
 \texttt{open14}              \\
\bottomrule
\end{tabular}
\end{table}

We believe that these failures do not stem from incorrect or problematic
filtering logic, but from the way we handled de-privileging processes in our
frontend (Section~\ref{subsubsec:spawning-exec}). By changing the
\ac{uid}/\acp{gid} of the filtered process, we invalidated assumptions made by
the \ac{ltp} test suite authors, which likely caused these tests to fail.

Unexpected \ac{gid} values are clearly the cause of the \texttt{getgroups01} and
\texttt{*gid*} tests, and likely caused the failures in the \texttt{open*} and 
\texttt{semctl06} tests. \texttt{open()} and \texttt{semctl()} system calls
access files and semaphores: both objects whose access is controlled by group
permissions. 

We hypothesise that the failures observed here relate to 
assumptions made about the groups that are allowed to access the associated
files/semaphores, although the documentation is missing entries for the failures
seen here \cite{LTPFailDocs}. 

Similarly, the man pages for \texttt{perf\_event\_open()} state that the system 
call can fail with the \texttt{EACCES} error when 
``\textit{unprivileged process may encounter this error: attaching to a 
process owned by a different user}'' \cite{perfEventOpen2}. 
This failure almost certainly stems from the manually changed \ac{uid}/\acp{gid}.

The \texttt{process\_vm\_readv()} and \texttt{process\_vm\_writev()} system calls 
allow for a form of \ac{ipc} which bypasses kernel space, so it is not immediately 
obvious why these tests fail \cite{process_vm_readv}. More work is needed to
identify the root cause of these failures.

Crucially, these failures are artifacts of the testing setup's de-privileging
mechanism. Using an alternative approach (e.g., a \texttt{setuid} helper
\cite{von2010ubuntu}) would
likely resolve these \ac{ltp} compatibility issues. Addressing this is left
for future work. The core filtering logic, validated manually, appears
correct and therefore we proceed with the security evaluation.

\subsection{Security Evaluation}\label{subsec:security-eval}

To evaluate \af's privilege reduction compared to seccomp, we define a
probabilistic privilege metric based on permitted system calls. \af filters 
system calls based on their origin within file backed memory regions 
(typically shared libraries) in the process's \ac{vma} 
(Section~\ref{subsec:objs-assumpions-tm}). We term these regions compartments. 
Our metric operates on these compartments. 

For comparison, a seccomp filtered  application is treated as a single
compartment encompassing all permitted system  calls. This unified approach
avoids needing separate metrics. Using this metric 
(Equation~\ref{eqn:privilege}) and our tool \texttt{syso}, we demonstrate
significant privilege reduction with \af across our benchmarks.
 
\subsubsection{Quantifying Privilege of
Compartments}\label{subsubsec:eval-quant-privilege}

To quantify privilege reduction, we first reason about what the 
\textit{privilege} of an application intuitively means and then look to ground
the notion mathematically. Our metric is \textit{probabilistic} -- it allows us
to reason about the likelihood of compromise. Probabilistic metrics are
precedented in systems security, for example entropy in \ac{aslr} \cite{aslr_entropy}.

We posit that the privilege of a component (in the context of system call
filtering) quantifies the likely potential harm it could cause via the system calls
it is permitted to execute. We considered whether the number of unique
system calls an application can make would be a sensible metric for privilege
but decided that this was imprecise. The reason why is
demonstrated by the following example:

Consider two compartments which allow an attacker access to
exactly one system call: attacker \textit{A} has access to the 
\texttt{getpid()} system call and attacker \textit{B} has access to the
\texttt{execve()} system call. Both have access to exactly one syscall, but
\texttt{execve()} is significantly more powerful than \texttt{getpid()}.
If we defined privilege as the number of unique system calls, each compartment
would have the same privilege level. Intuitively, this is not sensible, so we
defined our metric to factor in how ``powerful'' a system call is. 

To do this, we turned to existing system call literature to rank
how ``powerful'', or ``dangerous'' a given system call is. 
\textcite{SYSCALL_RANKINGS} classified syscalls into three different danger
levels, which we integrated into our analysis. In our metric, each   type of
system call receives a \textit{danger score} - $1$, $2$, or $3$ - according
to the privilege level of the system call.

We define the overall privilege score for a specific compartment, denoted as
$C$, by summing the individual danger scores of all system calls invoked
within that compartment. Let $S_C$ be the set of unique system calls present
in compartment $C$, and let $\mathrm{Score}(s)$ represent the danger score
assigned to a specific system call $s \in S_C$. The Compartment Privilege, $P_C
$, is then calculated as follows:  

\begin{equation} \label{eqn:privilege} 
    P_C = \sum_{s \in S_C} \mathrm{Score}(s) 
\end{equation} 

Here:
\begin{itemize}
    \item $P_C$ represents the total privilege score for compartment $C$.
    \item $S_C$ is the set of system calls invoked by compartment $C$.
    \item $s$ is an individual system call from the set $S_C$.
    \item $\mathrm{Score}(s)$ is the predefined danger score ($1, 2,$ or $3$)
        associated with the system call $s$.
\end{itemize}

Note that the concept of privilege is defined on compartments, not applications.
This raises an interesting question: how does one compare the privilege of
applications with a different number of compartments?

\subsubsection{Extending Privilege to Applications}\label{subsubsec:eval-quant-priv-apps}

Assigning a privilege score to an application $A$ with only one compartment $C
$ is trivial: the privilege of the application, $P_A$, is equal to the
privilege score of its single compartment, $P_C$ (as defined in 
Equation~\ref{eqn:privilege}). 

In the case of a multi-compartment application (practically all \af-protected
applications), with compartments $C_1, \dots, C_n$ and respective
privilege scores $P_{C_1}, \dots, P_{C_n}$, we define the privilege of the
application $P_A$ as the score of the \textit{most privileged compartment}: 

\begin{equation} \label{eqn:app-privilege} 
    P_A = \max_{j=1,\dots,n} \{ P_{C_j} \} 
\end{equation} 

For example, an application made up of four compartments, each
with compartment privilege scores ($P_C$) of $\{8, 19, 19, 23\}$ would be
assigned an application privilege score $P_A = 23$.

This definition (Equation~\ref{eqn:app-privilege}) assumes that an attacker who
has compromised the application has gained control of the most privileged
compartment. Therefore, the application's overall privilege level is
dictated by its weakest (most privileged) part. We use the worst case to
inform the application's privilege score as we cannot know how
likely a given compartment is to be compromised, so any attempts at
estimating an average score would be invalid. 

Therefore, the privilege
reduction estimates derived using this metric are also worst-case estimates;
\af will likely demonstrate greater privilege reduction in real-world
scenarios than the numbers reported here indicate. 

This allows us to assign specific application privilege scores based on the
filtering mechanism applied. Let $P_{A, \text{\af}}$ be the application
privilege score when filtered with \af (the ``\af score''), and 
$P_{A, \text{seccomp}}$ be the score when filtered with seccomp (the ``
seccomp score'').

For clarity, we provide a visualised example in
Figure~\ref{fig:pr}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{./diagrams/pr-syscall-distribution.drawio.pdf}
    \caption{A diagram showing system calls made by an application and how
    filtering with seccomp versus \af changes privilege levels}.
    \label{fig:pr}
\end{figure}


It is worth noting that $P_{A, \text{seccomp}} \ge P_{A, \af}$. This
relationship holds because the set of system calls permitted by the \af 
configuration for an application is, by construction, a subset of the system
calls permitted by the corresponding seccomp configuration in our setup.
Therefore, the system calls in the most permissive \af compartment will always
be a subset of the seccomp system calls, and therefore \af can only ever be as at
most as privileged as seccomp.

\subsubsection{Privilege Reduction in Benchmarks}

To compare \af and seccomp directly, we define the percentage of privilege
reduction achieved by \af compared to seccomp as per 
Equation~\ref{eqn:privilege-reduction}.  

\begin{equation}\label{eqn:privilege-reduction}
    \% \: \mathrm{Privilege Reduction} = 
    \frac{P_{A, \text{seccomp}} - P_{A, \text{\af}}}{P_{A, \text{seccomp}}} 
            \times 100
\end{equation}

We implemented an evaluation tool, \texttt{syso}, to calculate these privilege
reduction metrics. \texttt{syso} identifies which compartments are making
different system calls (in a similar way to \texttt{afgen}) and uses the
predefined system call set to calculate a score for each compartment, as
discussed in \ref{subsubsec:syso-impl}. It then takes the union of each
compartment system call list and calculates its privilege to
estimate the privilege of a seccomp filter.

\subsubsection{Results}

Table~\ref{tab:pr-results} shows the privilege reduction results from each benchmark.
Since \texttt{syso} is also a dynamic analysis-based tool, we ran each
benchmark three times with \texttt{syso} enabled and took the union of the set of
system calls attributed to each compartment across all three runs. This
increased our confidence that we were gathering a representative sample of
system calls without needing to invest significant engineering effort in a static
 analysis based tool. 

Running these tests generated lots of files and artefacts, which have all been
made publicly available in the GitHub repository linked in 
Appendix~\ref{apx:eval-artefacts}. We include the per-compartment system call counts 
(after a union was taken) that \texttt{syso} reported for each benchmark in
our evaluation artefacts repository (see Appendix~\ref{apx:eval-artefacts}).
We also provide the scripts we used to generate all plots and summary documents
within the repository.

\begin{table}[h] 
  \centering
  % Replaced \af with AF for robustness, assuming it is not a defined command
  \caption{Privilege reduction when using \af over seccomp for each benchmark}
  \label{tab:pr-results}
  % Updated column definitions: l (left), S (integer), S (integer), S (decimal)
  \begin{tabular}{l S[table-format=3.0] S[table-format=2.0] S[table-format=2.2]}
    \toprule
    % Updated header text and column alignment
    Benchmark & {Raw Seccomp Score} & {Raw \af Score} & {Percentage-PR (\%)} \\
    \midrule
    % Populated with corrected data
    Redis        & 119 & 53 & 55.46 \\
    Nginx        & 78  & 59 & 24.35 \\ % Used scores from 24.36% snippet
    \texttt{fio} & 45  & 20 & 55.56 \\ % Corrected PR from 55.46% based on snippets
    Postgres     & 93  & 71 & 23.66 \\
    NPB          & 38  & 18 & 52.63* \\ % Median scores used
    \bottomrule
  \end{tabular}
  \par\medskip\footnotesize
  % Corrected Min/Max in footnote
  *Median of 9 results; Min 42.11, Max 55.26
\end{table}

This set of results shows that filtering an application with \af
reduces the privilege of an attacker who has compromised an application. With
\af enabled, the attacker has access to a significantly smaller set of
``dangerous'' system calls when compared to seccomp. Three of our five
benchmarked applications (or 12 out of 15 benchmarked binaries) exhibit
privilege reductions of over 50\%. \af, when used as a filtering solution,
creates a significantly more secure environment for the applications it
protects.

We mention that 15 test binaries were run: this is because the \ac{npb} suite is
made of 9 individual binaries, therefore giving 9 different privilege reduction
scores. We opted to report the median result in Table~\ref{tab:pr-results}, but
include the percentage reductions in Table~\ref{tab:npb_individual_sorted} for 
completeness.

\begin{table}[ht]
  \centering
  \caption{Individual \ac{npb} Benchmark Privilege Reduction Results}
  \label{tab:npb_individual_sorted}
  % Using 'l' for benchmark name and 'S' for percentage alignment
  \begin{tabular}{l S[table-format=2.2]}
    \toprule
    Benchmark & {Privilege Reduction (\%)} \\ % {} protects header from siunitx
    \midrule
    is.C.x    & 55.26 \\ 
    ft.C.x    & 52.63 \\
    cg.C.x    & 52.63 \\
    ep.C.x    & 52.63 \\
    mg.C.x    & 52.63 \\
    bt.C.x    & 52.63 \\
    sp.C.x    & 52.63 \\
    lu.C.x    & 50.00 \\
    ua.C.x    & 42.11 \\ 
    \bottomrule
  \end{tabular}
\end{table}

Interestingly, Nginx and Postgres see more modest (but still significant) privilege reduction levels than the other benchmarks in our suite. We
investigated whether the number of shared libraries in a binary impacted the privilege reduction level. However, we found no correlation. The Postgres
binary has 42 shared libraries (measured with \texttt{ldd}) and Nginx has 8; 
both exhibit similar privilege reduction levels ($\sim$ 24\%). Redis, which
shows a 55\% privilege reduction, has 18 shared libraries. 

A more likely reason for this could be to do with the distribution of the system
calls within a binary. Applications showing higher privilege reduction scores
will likely have a more even distribution of system calls across their shared
libraries, whereas applications showing more modest reductions may have
``dangerous'' functionality concentrated in a few shared libraries.

Verifying this would be possible but would require static analysis and significant engineering work. While static approaches can provide
complete coverage, they are often complex and prone to over-approximation, 
especially in large, dynamic applications \cite{ernst2007daikon}. We
acknowledge that using dynamic analysis to estimate privilege reduction
is an inherent limitation of this evaluation, but we do not believe it
invalidates our results. Our use of multiple analysis runs and the
high privilege reduction scores seen indicate that \af is more secure than
seccomp. 

\subsection{Performance Evaluation}\label{subsec:perf-eval}

Having shown that \af effectively reduces the privilege of the applications that
it protects, we turn our attention to quantifying the runtime cost that \af
incurs. To do these, we ran our suite of benchmarks under the same
conditions as in Section~\ref{subsec:benchmark-selection}, but this time we
recorded the benchmark results. 

It is worth noting that due to \af's complexity, we expect to see some slowdown.
The seccomp filter that we installed is as minimal as possible; we are just comparing
the syscall number to a predefined allow list. Therefore, these benchmarks show
a best-case scenario for seccomp. On every system call, \af must unwind the
stack and walk the \ac{vma} red-black tree in the kernel. Therefore, we expect
to see some performance degradation in these benchmark results.

We conducted three experiments with each benchmark: in one experiment, no
application had a system call filter attached. One experiment was conducted with
a seccomp filter attached, and, finally, we attached \af for our final
experiment. Like this, we can show the absolute slowdown that \af
incurs by comparing with an unfiltered system, but also how \af compares to
industry-standard filtering tooling.

Each experiment was repeated three times and, as was the case for our Security
Analysis, all raw data, analysis scripts, and plots are available at the
repository linked in Appendix~\ref{apx:eval-artefacts}).

Each benchmark that we used is comprised of a set of micro-benchmarks. In
\textcite{HeiserBenchmarkingCrimes}, Heiser, a respected researcher in the
\ac{os} community argues that there is no good reason to report an arbitrary
subset of benchmark results. He goes further and argues that this is not good
academic practice and mentions that he often rejects papers for committing this
``benchmarking crime''. 

Therefore, we have chosen to plot our figures with all the benchmarking results
present but only highlight specific metrics measured in the benchmark. We
only present our comparisons between \af and seccomp in cases where \af
incurs significant runtime overhead. The exception here is how in the \ac{npb}
and \texttt{fio} benchmark results, where we report our comparison with the
baseline. Our reasoning for doing so is explained in 
Section~\ref{subsubsec:fio-slowdown}. 

This makes our results less clear to interpret than the Privilege Reduction
results in Table~\ref{tab:pr-results}, but we explain all
results as they are presented. Some benchmarks also have many tests
and metrics associated with them (e.g. Redis), which makes highlighting everything in this
report infeasible. Therefore, we urge the reader to look for more detail in our
artefacts repository if they feel the need to do so.

All plots have associated error bars (except percentage difference plots, which
are calculated with the mean average of the three runs). Error bars are
calculated using the standard error of the sample mean, 
$\frac{\sigma}{\sqrt n}$.

\subsubsection{Nginx Slowdown}\label{subsubsec:nginx-slowdown}

We visualise the performance cost of \af on Nginx in
Figure~\ref{fig:nginx-perf}. We see a (statistically significant) 25.1\%
reduction in throughput and a 36.1\% increase in average latency, shown in
Figure~\ref{fig:nginx-percdiff}. This is a more significant slowdown than
we saw with Redis. Identifying the exact cause of the slowdown is reserved
for future work, as it would involve profiling, trying different test
parameters, and more. 

One key difference in the way the tests were run was the concurrency level used
in Nginx: we simulated 200 client connections using four threads, as noted in
Section~\ref{subsubsec:nginx-just}. It could be that this high level of
concurrency stressed the operating system more (as Nginx worker threads may need
to be de-scheduled for \ac{os} upkeep) so than in Redis's case, which could
explain the increased latency and reduced throughput.

Nginx exhibited a privilege reduction of 25\% in Table~\ref{tab:pr-results}: as
such, Nginx may not benefit so much from \af, however as with all benchmark
results, this will depend on use case.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.75 \textwidth}
        \centering
        \includegraphics[width=\textwidth]{./evaluation-artefacts/results/nginx/plots/wrk_comparison_requests_per_second.pdf} 
        \caption{Nginx: comparison of transfers per second between \af and
        seccomp.}
        \label{fig:nginx-rps}
    \end{subfigure}
    \hfill
     \begin{subfigure}[b]{0.45 \textwidth}
        \centering
        % Choose the best latency plot, e.g.:
        \includegraphics[width=\textwidth]{./evaluation-artefacts/results/nginx/plots/wrk_comparison_latency.pdf} 
        \caption{Nginx: Comparison of average latency with \af versus
        seccomp}
        \label{fig:nginx-time}
    \end{subfigure}
     \medskip 
     \begin{subfigure}[b]{0.45 \textwidth}
        \centering
        \includegraphics[width=\textwidth]{./evaluation-artefacts/results/nginx/plots/wrk_comparison_percent_diff.pdf} % Check what this shows
        \caption{Percentage difference overhead between \af and seccomp}
        \label{fig:nginx-percdiff}
    \end{subfigure}

    \caption{Performance evaluation for the Nginx benchmark.}
    \label{fig:nginx-perf}
\end{figure}

\subsubsection{Postgres Slowdown}\label{subsubsec:postgres-slowdown}

Postgres shows slowdown (Figure~\ref{fig:pgs-perf}) somewhere between Redis and Nginx. \af incurs a 16\% reduction in transaction throughput and
a 19\% increase in average latency. Postgres exhibited a similar privilege
reduction to Nginx, around 25\%. 

The intermediate performance overhead observed for Postgres when filtered
by \af is likely primarily due to two things. Its inherently mixed
workload, combining network communication, \ac{cpu} processing, and frequent
disk \ac{io}, results in a moderate overall system call frequency. 

Secondly, we hypothesise that the frequent writes (as part of Postgres's
Write-Ahead Logging) may increase system call frequency and therefore slowdown
with \af enabled. \texttt{fio} - a benchmark which is focused on \ac{io} -
showed no slowdown (Section~\ref{subsubsec:fio-slowdown}). The key difference in
Postgres's case is that unlike the large block asynchronous writes
configured in our \texttt{fio} benchmark, this frequent synchronous \ac{io}
pattern introduces more frequent system calls and, therefore, increased overhead.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.45 \textwidth}
        \centering
        \includegraphics[width=\textwidth]{./evaluation-artefacts/results/postgres/plots/pgbench_comparison_tps.pdf} 
        \caption{Postgres: Comparison of transactions per second with \af
        and seccomp}
        \label{fig:pgs-txs}
    \end{subfigure}
    \hfill
     \begin{subfigure}[b]{0.45 \textwidth}
        \centering
        % Choose the best latency plot, e.g.:
        \includegraphics[width=\textwidth]{./evaluation-artefacts/results/postgres/plots/pgbench_comparison_latency.pdf} 
        \caption{Postgres: Comparison of average latency with \af versus
        seccomp}
        \label{fig:pgs-time}
    \end{subfigure}
     \medskip 
     \begin{subfigure}[b]{0.45 \textwidth}
        \centering
        \includegraphics[width=\textwidth]{./evaluation-artefacts/results/postgres/plots/pgbench_comparison_percent_diff.pdf} % Check what this shows
        \caption{Percentage difference overhead between \af and seccomp}
        \label{fig:pgs-percdiff}
    \end{subfigure}

    \caption{Performance evaluation for the Postgres benchmark.}
    \label{fig:pgs-perf}
\end{figure}

\subsubsection{Redis Slowdown}\label{subsubsec:redis-slowdown}

Figure~\ref{fig:redis-perf} indicates that \af incurs a performance cost when
run on Redis. Figure~\ref{fig:redis-rps} shows a significant reduction in
requests per second across all tests in the \texttt{redis-benchmark} suite
except the \texttt{LRANGE\_*} suite of tests. The median percentage slowdown in
throughput was 8.43\% in the \texttt{HSET} test.

We observed no significant difference in the 99th percentile response
latency in all test cases except the \texttt{*ADD} tests 
(Figure~\ref{fig:redis-p99}). We observe a spike in the \texttt{LRANGE\_600}
test case, but also see a very large error bar which overlaps with that of
seccomp's measured latency. Therefore, we do not conclude these measurements to
be significantly different.

Figure~\ref{fig:redis-perf} shows that every test case showed increased
average latency and decreased throughput when using \af instead of seccomp.
However, \textbf{percentage differences are almost always within $\pm 10\%$}.
This shows a modest slowdown and when contextualised with an almost 56\%
privilege reduction makes \af a compelling solution.

\begin{figure}[htbp]
    \centering
    % Subfigure 1: RPS (Assuming one plot shows all comparisons)
    \begin{subfigure}[b]{\textwidth}
        \centering
        % Choose the best RPS comparison plot, e.g.:
        \includegraphics[width=\textwidth]{./evaluation-artefacts/results/redis/scmp-plots/redis_comparison_rps.pdf} 
        \caption{Requests Per Second (RPS)}
        \label{fig:redis-rps}
    \end{subfigure}
    \hfill
    % Subfigure 2: Latency (e.g., P99)
     \begin{subfigure}[b]{\textwidth}
        \centering
        % Choose the best latency plot, e.g.:
        \includegraphics[width=\textwidth]{./evaluation-artefacts/results/redis/scmp-plots/redis_comparison_p99_latency_ms.pdf} 
        \caption{P99 Latency (ms)}
        \label{fig:redis-p99}
    \end{subfigure}
    
    % Maybe add the percent diff plot if available and clear
     \medskip 
     \begin{subfigure}[b]{1.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./evaluation-artefacts/results/redis/scmp-plots/redis_comparison_percent_diff.pdf} % Check what this shows
        \caption{Percentage difference overhead}
        \label{fig:redis-percdiff}
    \end{subfigure}

    \caption{Performance evaluation for the Redis benchmark.}
    \label{fig:redis-perf}
\end{figure}

\subsubsection{\texttt{fio} Slowdown}\label{subsubsec:fio-slowdown}

We have chosen to report our \texttt{fio} and \ac{npb} benchmark results directly
compared to baseline instead of with seccomp. This is because \af
sees \textbf{no significant slowdown, in any metric}, across the two sets of
benchmarks. 

\texttt{fio}'s benchmark results are visualised in Figure~\ref{fig:fio-perf}.
We have omitted the percentage difference plots as we saw a 0.0\%, 0.0\%, 0.0\%, and
(statistically insignificant) +2.7\% difference in IOPS throughput, average,
P99 and P99.99 latency, respectively. 

We speculate that \texttt{fio}'s performance was almost completely
unaffected by \af likely because we used a block size of 1MB when
configuring \texttt{fio}. Using very large block sizes reduces the number of
system calls made during the writing. Since \af incurs overhead on each
system call, making few system calls implies low overhead. Another reason
may be that \texttt{fio} is bottlenecked by \ac{io} instead of \ac{cpu} or
the \ac{os}. Therefore, making system calls slower does not change performance
numbers much (as a percentage) because we are not affecting the bottleneck.

This indicates that \af will likely be well-suited to protect applications with
a lot of \ac{io} activity.

\begin{figure}[htbp]
    \centering
    % Subfigure 1: IOPS
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./evaluation-artefacts/results/fio/plots/fio_comparison_iops.pdf}
        \caption{IOPS comparison}
        \label{fig:fio-iops}
    \end{subfigure}
    \hfill % Space between horizontal figures
    % Subfigure 2: Latency
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./evaluation-artefacts/results/fio/plots/fio_comparison_latency.pdf}
        \caption{Average latency comparison}
        \label{fig:fio-latency}
    \end{subfigure}

    \medskip % Vertical space between rows
    
    \caption{Performance evaluation for the \texttt{fio} benchmark, comparing no
    filter with \af{}.} % Main figure caption
    \label{fig:fio-perf}
\end{figure}



\subsubsection{\ac{npb} Slowdown}\label{subsubsec:npb-slowdown}

As mentioned in Section~\ref{subsubsec:fio-slowdown}, we present the results
from the \ac{npb} benchmark suite as a comparison between \af and no filtering
solution, as we see no significant difference between the baseline and \af (see
Figure~\ref{fig:npb-perf}).

The reasons are likely different in \ac{npb} than in \texttt{fio}. \ac{npb}
spends an overwhelming amount of time doing arithmetic calculations \textit{in
user space} - therefore, it is not reliant on system call related behaviour.
Calls will be made for things like thread synchronisation and memory 
operations (e.g.\texttt{futex()},{}\texttt{mmap()}, etc\dots). This likely
leads to a low system call frequency and, therefore, no significant overhead.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.7 \textwidth}
        \centering
        \includegraphics[width=\textwidth]{./evaluation-artefacts/results/nasa/base-plots/npb_comparison_mops.pdf} 
        \caption{\ac{npb} Millions of Operations per Second (\af vs no filter)}
        \label{fig:npb-mops}
    \end{subfigure}
    \hfill
     \begin{subfigure}[b]{0.7 \textwidth}
        \centering
        % Choose the best latency plot, e.g.:
        \includegraphics[width=\textwidth]{./evaluation-artefacts/results/nasa/base-plots/npb_comparison_time.pdf} 
        \caption{\ac{npb} Time taken to run each micro-benchmark (\af vs no
        filter)}
        \label{fig:npb-time}
    \end{subfigure}
     \medskip 
     \begin{subfigure}[b]{0.7 \textwidth}
        \centering
        \includegraphics[width=\textwidth]{./evaluation-artefacts/results/nasa/base-plots/npb_comparison_percent_diff.pdf} % Check what this shows
        \caption{Percentage difference overhead}
        \label{fig:npb-percdiff}
    \end{subfigure}

    \caption{Performance evaluation for the \ac{npb} benchmark suite.}
    \label{fig:npb-perf}
\end{figure}

This evaluation demonstrates that \af can significantly enhance
application security by reducing privilege far beyond baseline seccomp
configurations, achieving measured reductions between 24\% and 56\% across
our benchmark suite using our defined metric. Our validation
confirms that the core filtering mechanism operates as intended, although further
investigation is required to address compatibility issues highlighted by
specific \ac{ltp} test failures. We have shown that \af is not only effective
in securing applications, but also quantified the slowdown that users can 
expect to see when using \af.

\subsection{Analysis of Performance Overhead}

To understand the source of the performance variations observed across the
benchmarks, we analysed profiling data gathered from \af's filtering logic,
(see Section~\ref{subsubsec:impl-find-site-opt}). We compare the time spent in
each execution stage of the core filtering logic across applications to
understand why some apps had more performance overhead than others.

We gathered profiles from each application, removed outliers (defined as
$\pm1.5\times$ \ac{iqr}), and took a random sample of 1,000 system calls. 
Figure~\ref{fig:profile-means} shows the mean time spent in each stage of
filtering in nanoseconds. Individual flame graphs for each benchmark are 
provided in Appendix~\ref{apx:flame-graphs}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{./evaluation-artefacts/results/profiling/mean_stacked_horizontal_comparison.pdf}
    \caption{A stacked bar chart showing the average time (in nanoseconds) \af
    spent in each phase of filtering.}
    \label{fig:profile-means}
\end{figure}

We see that \af processes Redis's sytem calls significantly quicker than the
other benchmarks (around 2000ns), and Nginx's significantly slower (almost
8000ns). This explains why Redis only saw moderate slowdown despite being so 
system call intensive. Postgres, \texttt{fio} and the \texttt{bt.C.x} \ac{npb}
microbenchmark each took around 5000ns on average to process a system call.

Redis seems to see the greatest time saving in the system call site
identification, which is where stack unwinding occurs. We hypothesise that Redis
may have shallower stacks than the other applications tested here. This would
imply that we reach the bottom of the stack before all 16 frames are pulled
(Section~\ref{subsec:impl-syscall-site}) and explains a significant portion of the 
speedup.

\af takes proportionally longer to decide whether to filter Nginx's processes
and apply fork following than the other benchmarks. This could be because we
stressed Nginx's concurrency -- as mentioned, Nginx forks on receiving a
request. More forks could mean more fork following operations, which mean lots
of writes to a (synchronised) \ac{bpf} map.

We observe that \ac{npb} and \texttt{fio} took a similar time to Postgres
despite seeing no performance overhead in testing. This is because 
\texttt{fio} and \ac{npb} tests simply made far fewer system calls: 84,374 over
30 seconds and 64,791 over 38 seconds respectively, compared to 3,028,544 made 
by Postgres in 30 seconds.

Further verification work, for example logging the number of stack frames
per unwinding operation to test the Redis hypothesis, is needed. 
Additionally, performance engineering efforts could potentially reduce some of the
overheads reported, particularly for applications like Nginx and Postgres.
