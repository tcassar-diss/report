\section{Evaluation}\label{sec:evaluation}

In this section we look to show that \af is able to significantly reduce the
privilege of the applications that it protects -- often more so than seccomp. 
We quantify the additional runtime cost that developers can expect from using 
\af  instead of seccomp, and look to prove the correctness of \af.

We start by sharing our test environment and the version of \af we used to
aid reproducibility. We then move on to showing that \af is correct: that
is, it correctly identifies which shared library made which system call without
affecting normal system call function. Having shown correctness, we move on to
security evaluation, where we define a privilege reduction metric and apply the
metric to a set of application benchmarks. We finish with performance analysis,
where we compare the runtime cost of \af to that of using a seccomp filter.

\subsection{Test Environment}

Our test environment was the same as our development environment outlined in
Section~\ref{subsection:tech-stack}. We used a Linux server (kernel version
6.8.52-generic) running Ubuntu 24.04.2 with a two Intel Xeon
Gold 5220R x86 processors clocked at 2.20GHz. Each socket has 24 physical 
cores with two threads per core, resulting in a total of 96 logical cores.

The server has 64 GB of memory configured at 2666 MT/s with 2 terabytes
of storage; buffered read performance was measured at 515 MB/s with 
\texttt{sudo hdparm -t /dev/sda}.

The kernel is being run with default settings - \ac{aslr}, stack
canaries, and other default security features remained enabled for all tests.
\af was run as root, but as mentioned in Section~\ref{subsubsec:spawning-exec},
filtered applications were not run with root privileges.

\af was compiled with the Go compiler at version 1.23.2 with no flags or
environment variables explicitly set. \ac{bpf} was
compiled using the \texttt{ebpf-go} toolchain from the Cilium project at version
v0.17.1. Under the hood, this relies on LLVM: our server was configured with
clang version 18.1.3. We used \af release version 1.0.0, which can be found on
GitHub at
\href{https://github.com/tcassar-diss/addrfilter/releases/tag/v1.0}
{www.github.com/tcassar-diss/addrfilter/releases/tag/v1.0}
(commit hash 
\href{https://github.com/tcassar-diss/addrfilter/tree/2bd209c630df3509d4ac721d018dabab94305dde}
{\texttt{2db209c}}).

\begin{table}[h] % Position specifier (here, top, bottom, page)
    \centering % Center the table
    \caption{Test Environment Configuration Summary}
    \label{tab:test_environment}
    \begin{tabular}{@{}ll@{}} % Left-aligned columns, @{} removes side padding
        \toprule
        Component           & Specification \\
        \midrule
        \multicolumn{2}{@{}l}{\textbf{System Hardware \& OS}} \\ 
        \aclp{cpu}           & 2x Intel Xeon Gold 5220R \\
        Operating System    & Ubuntu 24.04.2 \\
        Kernel              & Linux 6.8.52-generic \\
        Logical Cores       & 96 (2 sockets x 24 cores/socket x 2 threads/core) \\
        System Memory       & 64 GB @ 2666 MT/s \\
        Storage             & 2 TB (Buffered Read: \textasciitilde{}515 MB/s via \texttt{hdparm}) \\
        Kernel Security     & Standard defaults enabled (ASLR, Stack Canaries, etc.) \\
        \midrule
        \multicolumn{2}{@{}l}{\textbf{Software \& Build Environment}} \\ 
        Software Under Test & \texttt{addrfilter} v1.0.0 (Commit: \texttt{2db209c}) \\
        Go Compiler         & Version 1.23.2 \\
        eBPF Toolchain      & Cilium ebpf-go v0.17.1 \\
        LLVM Backend (for eBPF) & Clang version 18.1.3 \\
        \midrule
        \multicolumn{2}{@{}l}{\textbf{Benchmarking Configuration}} \\ 
        Network Tests       & Client and Server processes co-located on host \\
        Application Privileges & Filtered applications run as non-root \\
        \bottomrule
    \end{tabular}
\end{table}

Key aspects of the test hardware and software are summarised in
Table~\ref{tab:test_environment}.
For experiments involving data transfer over a network (such as
\texttt{redis-benchmark} and \texttt{wrk} for Nginx), the server and benchmark
client were both run on the same server to avoid the network becoming a bottleneck.

While experiments were performed on server-grade hardware, the results should be
generalisable to user-oriented systems. Benchmark results are always reported
with \af enabled and disabled to help the reader understand relative
performance differences.

\subsection{Benchmark Selection}\label{subsec:benchmark-selection}

To evaluate the efficacy and overhead of a \af, we tested against a broad set
of workloads. Both security and performance analysis were conducted on a 
benchmark suite comprising five distinct applications and workloads,
aiming to cover common server application paradigms. The version of each
benchmark/application can be found in 
Table~\ref{tab:benchmark_software_versions}. Collectively, these benchmarks were
selected to stress \af across a wide system call profile: network-dominated 
(Redis, Nginx), \ac{io}-intensive (\texttt{fio}), \ac{cpu}-intensive 
(\ac{npb}), and complex, mixed workloads (PostgreSQL).

\begin{table}[h] 
    \centering 
    \caption{Benchmark Suite Software Components and Versions}
    \label{tab:benchmark_software_versions}
    \begin{tabular}{@{}ll@{}} % Left-aligned columns, @{} removes side padding
        \toprule
        Software Component    & Version \\
        \midrule
        Redis Server          & 7.0.15 \\
        \texttt{redis-benchmark} & 7.0.15 \\
        Nginx                 & 1.24.0 \\
        \texttt{wrk} (Load Generator) & 4.1.0 \\ % Added context for wrk
        \texttt{fio} (I/O Tool) & 3.39-38-gf18c \\ % Added context for fio
        PostgreSQL            & 16.8 \\
        NPB Suite             & 3.4.3 \\ % Assuming this is the NAS Parallel Benchmarks suite based on common usage
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Redis and \texttt{redis-benchmark}}\label{subsubsec:redis-just}

Redis is an in-memory key/value store, representing network-bound applications 
sensitive to latency. Its workload primarily stresses network stack system calls 
(\texttt{accept()}, \texttt{read()}, \texttt{write()}, \texttt{epoll\_wait}, 
etc.) and memory management, allowing us to  measure  \af's impact on high 
throughput, low latency network services. We used small key sizes (3B) to 
maximise the system call rate relative to data transfer, which will stress \af as 
much as possible.

During benchmarking, Redis was run with default configuration settings to
reflect a standard, no-tuning deployment. \texttt{redis-benchmark} was also run
with default settings: its small key size of 3 bytes makes the benchmark more
system call intensive, which has the effect of stressing \af as more syscalls are
made per second with small key sizes. We specified the number of clients that
\texttt{redis-benchmark} was to simulate with \texttt{-c 48} flag. This used
half the available cores on our server and was done to add a moderate amount of
concurrency to the benchmark in the hope of simulating something closer to
real-world conditions.

\subsubsection{Nginx and \texttt{wrk}}\label{subsubsec:nginx-just}

Nginx is a widely used high-performance web server, representative of applications
which deal with massively concurrent network connections and potentially
significant file \ac{io} (e.g., serving static assets). This benchmark exercises
network system calls, file system syscalls (open, read, sendfile),
and process/thread management, testing \af's overhead under high concurrency
and mixed \ac{io} patterns.

For Nginx, we ran our benchmark with some changes from the
default configuration. The \texttt{nginx.conf} we used is available in
Listing~\ref{lst:nginx-conf} (in Appendix~\ref{subsec:nginx}). Some key 
decisions were to set \texttt{worker\_processes} to auto to use available cores 
efficiently, and to enable \textit{sendfile} to let Nginx use the optimised 
\texttt{sendfile()} system call to serve our single-page static site (see 
Listing~\ref{lst:index-html}).

We disabled access logging to remove noise from disk \ac{io} to avoid stressing
the filesystem (as this was the purpose of the \texttt{fio} benchmark).
Loads were generated with \texttt{wrk} configured to use 200 concurrent
connections across four client threads (-c 200 -t 4) for a duration of 30
seconds. These parameters imposed significant concurrency, stressing Nginx's
connection and process/thread management subsystems.

\subsubsection{\texttt{fio}}\label{subsubsec:fio-just}
\texttt{fio}  is a flexible \ac{io} workload generator used here to estimate
the impact of \af on the storage subsystem. By configuring \texttt{fio} for
specific \ac{io} pattern, we can observe the runtime cost incurred by \af on
filesystem and block device related system calls. We carefully configured
\texttt{fio} to focus specifically on high throughput sequential write
performance, as data ingestion/writing large files to disk are often 
predominantly sequential write tasks.
% TODO: Cite that writing to disk is predominantly sequential write

% TODO: cite fio docs for ramp time
We used \texttt{fio} to write 10GB files per job (--size=10G) across 16 parallel jobs 
(--numjobs=16).  The benchmark was time-based, running for 5 minutes 
(--runtime=5m) after a 2-second ramp-up (--ramp\_time=2s) to ensure stable 
measurements. We enabled Direct \ac{io} (--direct=1) to bypass the
operating system's page cache. This ensures that write operations translate 
directly into \texttt{write()} system calls, making the benchmark highly sensitive 
to any overhead introduced by \af. A large block size of 1MB (--bs=1M) was 
used as it is characteristic of sequential throughput workloads. To maximize 
load and attempt to saturate the storage device, a high \ac{io} queue depth of 64
was used per job (--iodepth=64, --iodepth\_batch\_submit=64,
--iodepth\_batch\_complete\_max=64). This deliberately stresses the storage 
subsystem under heavy, parallel, direct write load, providing insight into 
\af's performance cost under such conditions.

\subsubsection{PostgreSQL}\label{subsubsec:psql-just}

PostgreSQL is a widely used, highly complex, relational database system which is
used here to represent a database workload. \texttt{pgbench} simulates
transactional workloads involving client connections (stressing the network
stack), query processing (\ac{cpu}/memory intense), and write-ahead logging
(sequential, synchronous disk \ac{io} via \texttt{write()} and \texttt{fsync()}
system calls). PostgreSQL's complexity and diversity lets us evaluate \af's
performance on a broad set of system calls.x` `

To exercise PostgreSQL, we used its standard benchmarking tool, 
\texttt{pgbench}. The Postgres server was started manually (i.e. not via
\texttt{systemd}) with default configuration. For load generation, we configured 
\texttt{pgbench} to simulate 48 concurrent clients (\texttt{-c 48}), each
corresponding to a worker thread (\texttt{-j 48}, matching \texttt{-c}).
This uses half the available logical cores (96) on our
test server, chosen to impose a significant transactional load (similar to
the rationale for Redis) without the risk of stressing system resources like the
\ac{cpu} scheduler. As with Nginx, each test was run for a duration of 30
seconds (\texttt{-T 30}). Each run was repeated three times.

\subsubsection{The \ac{npb} Suite}\label{subsubsec:npb-just}

The \acl{npb} suite of benchmarks is a set of computationally intensive 
benchmarks derived from computational fluid dynamics applications. This 
workload is inherently \ac{cpu}-bound, with fewer \ac{io} or network system calls
compared to the other selected benchmarks. This allows us to measure the
overhead on \af on compute-heavy processes, where system calls will primarily relate
to thread management (e.g. \texttt{futex()}) and memory management (e.g. 
\texttt{mmap()}).

We used an entirely standard configuration for the \ac{npb} suite, with our
\texttt{make.def} and \texttt{suite.def} (files used to define the benchmark
suite) provided in Listings~\ref{lst:npb-make-def} and \ref{lst:npb-suite-def}
respectively. We opted to use the OpenMP based test suite, with the workload 
size set to \texttt{C} (a standard \ac{npb} problem size) as this was enough 
work for a stable, accurate reading while ensuring a single run of the test 
suite did not take more than three minutes.

% Redis version 7.0.15 with \texttt{redis-benchmark} version 7.0.15
% Nginx version 1.24.0 \texttt{wrk} version 4.1.0
% \texttt{fio} \textbf{version 3.39-38-gf18c}
% PostgreSQL version 16.8
% version 3.4.3

\subsection{Validation}

To validate that \af works correctly, we used a combination of strategies. We 
extensively used manual testing on simple \textit{"Hello, World"} style 
applications during development to ensure that the filtered application was 
killed when it made a non-whitelisted system call. We then wrote a collection of 
shared libraries (Listings \ref{lst:getpid-c},~\ref{lst:printf-c}) containing 
functions that wrapped \ac{libc} function, and rewrote our 
\textit{"Hello, World"} style application (provided in Appendix~\ref{sec:linkage})
to use them. This example lets us easily reason about where we expect system calls
to come from. We expect the \texttt{write()} system call (number 1) to come from 
the \texttt{printf.so} and the \texttt{getpid()} (number 39) to come from 
\texttt{getpid.so}. We generated a whitelist with \af \texttt{generate} 
(Listing~\ref{lst:whitelist-toml}) and saw system calls 1 and 39 originating from
their expected shared library files.

When we manually removed a system call from the shared library's whitelist, we 
observed that the process was killed. This  verified that \af works correctly
on small examples.

Having shown that \af was able to detect and kill processes making system calls not
on their whitelists, we then ensured that system call behaviour had not been 
adversely affected by \af. To do this, we turned to the \acg{ltp} system call test 
suite \cite{LINUX_TEST_PROJECT}. \ac{ltp} was installed according to the
\href{https://linux-test-project.readthedocs.io/en/latest/users/quick_start.html}{documentation}.

We ran the system call test suite using the command \texttt{/opt/ltp/kirk -f ltp -r
system calls} with no filtering enabled for some baseline results. We then ran
\texttt{afgen /opt/ltp/kirk}\dots to generate the relevant
whitelists. We supplied this whitelist as an argument to addrfilter, and ran
\texttt{addrfilter ./whitelist.json /opt/ltp/kirk}\dots to run the test suite
with addrfilter enabled. We observed differences in test failures with \af
enabled versus disabled, displayed in Table~\ref{tab:af_failures}.

\begin{table}[h]
\centering
\caption{\ac{ltp} test failures with \af disabled and enabled.}
\label{tab:af_failures}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{\af disabled} & \textbf{\af enabled} \\
\midrule
\texttt{listmount04}       & \texttt{getegid02}           \\
\texttt{fanotify13}        & \texttt{getgid03}            \\
\texttt{statmount02}       & \texttt{listmount04}         \\
\texttt{statmount06}       & \texttt{process\_vm\_readv02} \\
                           & \texttt{process\_vm\_readv03} \\
                           & \texttt{process\_vm\_writev02}\\
                           & \texttt{getegid02\_16}        \\
                           & \texttt{getgroups01}         \\
                           & \texttt{open14}              \\
                           & \texttt{openat03}            \\
                           & \texttt{semctl06}            \\
                           & \texttt{perf\_event\_open01} \\
\bottomrule
\end{tabular}
\end{table}

This difference is a cause for concern, as our solution should not break
existing kernel functionality. Some failures are explainable, and some require
future research. We hypothesize that the failures relating to \textit{groups}
(\texttt{getegid02}, \texttt{getgid03}, \texttt{getegid02\_16},
\texttt{getgroups01}) are likely to do with the way the \af frontend spawns the
filtered application. In Section~\ref{subsec:impl-frontend} we discussed the
need to spawn applications without root privileges and achieved this by manually
changing the process's \ac{uid} and \ac{gid}. This manual changing of \acp{uid}
and \acp{gid} may change the results expected by the \ac{ltp} suite and be the
cause of the breakage.

%TODO: cite https://man7.org/linux/man-pages/man2/process_vm_readv.2.html
% TODO: cite https://man7.org/linux/man-pages/man2/perf_event_open.2.html
The \texttt{process\_vm\_readv()} and \texttt{process\_vm\_writev()} system calls 
allow for a form of \ac{ipc} which bypass kernelspace, so it is not immediately 
obvious why these tests fail. The same goes for the \texttt{semctl()} system call
which is used to control semaphores and the \texttt{perf\_event\_open()}
system call, used to set up performance monitoring: \af's design does not provide any
clues as to why these system calls tests fail.

The most puzzling of all are the failures in tests relying on the 
\texttt{open()} and \texttt{openat()} system calls. Both involve file creation -
throughout the development and evaluation process, filtered applications have
exhibited no issues with file interaction. It is possible that altering the
\ac{uid}/\ac{gid} of the filtered application may create a file associated with
a user/group unexpected by the \ac{ltp} test suite, but this is speculation.
More work is needed to identify the cause of these failures.

These 12 \ac{ltp} failures notwithstanding, we believe that the core filtering
mechanism functions as intended. Now, we move on to \textbf{security analysis},
where we quantify the degree of privilege reduction provided by \af.

\subsection{Security Evaluation}\label{subsec:security-eval}

To evaluate the privilege reduction afforded by using \af over seccomp, we
define a \textit{privilege metric} (Equation~\ref{eqn:privilege}), which is
a function of the set of system calls a compartment in an application can make.

Recall that \af provides fine-grained filtering based on file-backed
regions (shared libraries) within the process's \ac{vma}. \af associates a 
system call whitelist with each of these regions in accordance with our threat
model presented in Section~\ref{subsec:threat-model}. For the purpose of this
evaluation, we will refer to these regions as compartments. We chose to
define our metric in terms of these compartments, as this aligns with how \af
views the world. To calculate the privilege of a seccomp-filtered application,
we define the application as being made of a single compartment. 

This avoids the need to define two metrics, or ``special case'' 
seccomp-filtered applications.  Using this metric, and our custom evaluation
tool \texttt{syso}, we show that \af provides significant privilege
reduction compared to seccomp  for all applications and benchmarks we tested,
therefore fulfilling its requirement to reduce privilege\todo{reference exact req.}.
 
\subsubsection{Quantifying Privilege of
Compartments}\label{subsubsec:eval-quant-privilege}

To quantify privilege reduction, we first reason about what the 
\textit{privilege} of an application intuitively means and then look to ground
the notion mathematically.

We posit that the privilege of a component (in the context of system call
filtering) quantifies the potential harm it could cause via the system calls
it is permitted to execute. We considered whether the number of unique
system calls an application can make would be a sensible metric for privilege
but decided that this was imprecise. The reason why is
demonstrated by the following example:  

Consider two compartments which allow an attacker access to
exactly one system call: attacker \textit{A} has access to the 
\texttt{getpid()} system call and attacker \textit{B} has access to the
\texttt{execve()} system call. Both have access to exactly one syscall, but
\texttt{execve()} is significantly more powerful than \texttt{getpid()}.
If we defined privilege as the number of unique system calls, then each compartment
would have the same privilege level. Intuitively, this is not sensible, so we
defined our metric to factor in how ``powerful'' a system call is. 

To do this, we turned to existing system call literature to rank
how ``powerful'', or ``dangerous'' a given system call is. 
\textcite{SYSCALL_RANKINGS} classified syscalls into three different danger
levels, which we integrated into our analysis. In our metric, each type of
system call receives a \textit{danger score} - $1$, $2$, or $3$ - according
to the privilege level of the system call.

We define the overall privilege score for a specific compartment, denoted as
$C$, by summing the individual danger scores of all system calls utilised
within that compartment. Let $S_C$ be the set of unique system calls present
in compartment $C$, and let $\mathrm{Score}(s)$ represent the danger score
assigned to a specific system call $s \in S_C$. The Compartment Privilege, $P_C
$, is then calculated as follows:  

\begin{equation} \label{eqn:privilege} 
    P_C = \sum_{s \in S_C} \mathrm{Score}(s) 
\end{equation} 

Here:
\begin{itemize}
    \item $P_C$ represents the total privilege score for compartment $C$.
    \item $S_C$ is the set of system calls invoked by compartment $C$.
    \item $s$ is an individual system call from the set $S_C$.
    \item $\mathrm{Score}(s)$ is the pre-defined danger score ($1, 2,$ or $3$)
        associated with system call $s$.
\end{itemize}

Note that the concept of privilege is defined on compartments, not applications.
This raises an interesting question: how does one compare the privilege of
applications with a different number of compartments?

\subsubsection{Extending Privilege to Applications}\label{subsubsec:eval-quant-priv-apps}

Assigning a privilege score to an application $A$ with only one compartment $C
$ is trivial: the privilege of the application, $P_A$, is equal to the
privilege score of its single compartment, $P_C$ (as defined in 
Equation~\ref{eqn:privilege}). 

In the case of a multi-compartment application (practically all \af-protected
applications), with compartments $C_1, \dots, C_n$ and respective
privilege scores $P_{C_1}, \dots, P_{C_n}$, we define the privilege of the
application $P_A$ as the score of the \textbf{most privileged compartment}: 

\begin{equation} \label{eqn:app-privilege} 
    P_A = \max_{j=1,\dots,n} \{ P_{C_j} \} 
\end{equation} 

For example, an application made up of four compartments each
with compartment privilege scores ($P_C$) of $\{8, 19, 19, 23\}$ would be
assigned an application privilege score $P_A = 23$.

This definition (Equation~\ref{eqn:app-privilege}) assumes that an attacker who
has compromised the application has gained control of the most privileged
compartment. Therefore, the application's overall privilege level is
dictated by its weakest (most privileged) part. We use the worst case to
inform the application's privilege score as we have no way of knowing how
likely a given compartment is to be compromised, so any attempts at
estimating an average score would be invalid. Therefore, the privilege
reduction estimates derived using this metric are also worst case estimates;
\af will likely demonstrate greater privilege reduction in real-world
scenarios than the numbers reported here indicate. 

This allows us to assign specific application privilege scores based on the
filtering mechanism applied. Let $P_{A, \text{\af}}$ be the application
privilege score when filtered with \af (the ``\af score''), and 
$P_{A, \text{seccomp}}$ be the score when filtered with seccomp (the ``
seccomp score''). 

It is worth noting that $P_{A, \text{seccomp}} \ge P_{A, \af}$. This
relationship holds because the set of system calls permitted by the \af 
configuration for an application is, by construction, a subset of the system
calls permitted by the corresponding seccomp configuration in our setup.
Therefore, the system calls in the most permissive \af compartment will always
be a subset of the seccomp system calls, and therefore \af can only ever be as at
most as privileged as seccomp.

\subsubsection{Privilege Reduction in Benchmarks}

To compare \af and seccomp directly, we define the percentage privilege
reduction achieved by \af compared to seccomp as per 
Equation~\ref{eqn:privilege-reduction}.  

\begin{equation}\label{eqn:privilege-reduction}
    \% \: \mathrm{Privilege Reduction} = 
    \frac{P_{A, \text{seccomp}} - P_{A, \text{\af}}}{P_{A, \text{seccomp}}} 
            \times 100
\end{equation}

We implemented an evaluation tool, \texttt{syso}, to calculate these privilege
reduction metrics. \texttt{syso} identifies which compartments are making
different system calls (in a similar way to \texttt{afgen}), and uses the
predefined system call set to calculate a score for each compartment, as
discussed in \ref{subsubsec:syso-impl}. It then takes the union of each
compartment system call list, and calculates the privilege of that to
estimate the privilege of a seccomp filter.

\subsubsection{Results}

The privilege reduction results are tabulated in Table~\ref{tab:pr-results}.
Since \texttt{syso} is also a dynamic analysis based tool, we ran each
benchmark three times with \texttt{syso} enabled and unioned the set of
system calls attributed to each compartment across all three runs. This
increased our confidence that we were gathering a representative sample of
system calls without needing to invest significant engineering effort in a static
-analysis based tool. 

Running these tests generated lots of files and artefacts, which have all been
made publicly available in the GitHub repository linked in 
Appendix~\ref{apx:eval-artefacts}. We include the per-compartment system call counts 
(after a union was taken) that \texttt{syso} reported for each benchmark in
our evaluation artefacts repository (see Appendix~\ref{apx:eval-artefacts}).
We also provide the scripts we used to generate all plots and summary documents
within the repository.

\begin{table}[h] 
  \centering
  % Replaced \af with AF for robustness, assuming it's not a defined command
  \caption{Privilege reduction when using \af over seccomp for each benchmark}
  \label{tab:pr-results}
  % Updated column definitions: l (left), S (integer), S (integer), S (decimal)
  \begin{tabular}{l S[table-format=3.0] S[table-format=2.0] S[table-format=2.2]}
    \toprule
    % Updated header text and column alignment
    Benchmark & {Raw Seccomp Score} & {Raw \af Score} & {Percentage-PR (\%)} \\
    \midrule
    % Populated with corrected data
    Redis        & 119 & 53 & 55.46 \\
    Nginx        & 78  & 59 & 24.35 \\ % Used scores from 24.36% snippet
    \texttt{fio} & 45  & 20 & 55.56 \\ % Corrected PR from 55.46% based on snippets
    Postgres     & 93  & 71 & 23.66 \\
    NPB          & 38  & 18 & 52.63* \\ % Median scores used
    \bottomrule
  \end{tabular}
  \par\medskip\footnotesize
  % Corrected Min/Max in footnote
  *Median of 9 results; Min 42.11, Max 55.26
\end{table}

This set of results shows that filtering an application with \af significantly
reduces the privilege of an attacker who has compromised an application. With
\af enabled, the attacker has access to a significantly smaller set of
``dangerous'' system calls when compared to seccomp. Three out of our five
benchmarked applications (or 12 out of 15 benchmarked binaries) exhibit
privilege reductions of over 50\%. \af, when used as a filtering solution,
creates a significantly more secure environment for the applications it
protects.

We mention that 15 test binaries were run: this is because the \ac{npb} suite is
made of 9 individual binaries, therefore giving 9 different privilege reduction
scores. We opted to report the median result in Table~\ref{tab:pr-results}, but
include the percentage reductions in Table~\ref{tab:npb_individual_sorted} for 
completeness.

\begin{table}[ht]
  \centering
  \caption{Individual \ac{npb} Benchmark Privilege Reduction Results}
  \label{tab:npb_individual_sorted}
  % Using 'l' for benchmark name and 'S' for percentage alignment
  \begin{tabular}{l S[table-format=2.2]}
    \toprule
    Benchmark & {Privilege Reduction (\%)} \\ % {} protects header from siunitx
    \midrule
    is.C.x    & 55.26 \\ 
    ft.C.x    & 52.63 \\
    cg.C.x    & 52.63 \\
    ep.C.x    & 52.63 \\
    mg.C.x    & 52.63 \\
    bt.C.x    & 52.63 \\
    sp.C.x    & 52.63 \\
    lu.C.x    & 50.00 \\
    ua.C.x    & 42.11 \\ 
    \bottomrule
  \end{tabular}
\end{table}

Interestingly, Nginx and Postgres see more modest (but still significant) levels
of privilege reduction when compared to the other benchmarks in our suite. We
investigated to see if the number of shared libraries in a binary had an impact
on the privilege reduction level, however we found no correlation. The Postgres
binary has 42 shared libraries (measured with \texttt{ldd}) and Nginx has 8; 
both exhibit similar privilege reduction levels ($\sim$ 24\%). Redis, which
shows 55\% privilege reduction has 18 shared libaries. 

A more likely reason for this could be to do with the distribution of system
calls within a binary. Applications showing higher privilege reduction scores
will likely have a more even distribution of system calls across their shared
libraries, whereas applications showing more modest reductions may have
``dangerous'' functionality concentrated into few shared libraries.

Verifying this would be possible, but would require static analysis and
therefore significant engineering work. We acknowledge that the use of dynamic
analysis to estimate privilege reduction is an inherent limitation of this
evaluation, but we do not believe it invalidates our results. Our use of
multiple analysis runs, as well as the high privilege reduction scores seen,
indicate that \af is more secure than seccomp. 

\subsection{Performance Evaluation}\label{subsec:perf-eval}

Having shown that \af effectively reduces the privilege of the applications that
it protects, we turn our attention to quantifying the runtime cost that \af
incurs. To do these, we ran our suite of benchmarks under the same
conditions as in Section~\ref{subsec:benchmark-selection}, but this time we
recorded the benchmark results. 

We conducted three experiments with each benchmark: in one experiment, no
application had a system call filter attached. One experiment was conducted with
a seccomp filter attached and, finally, we attached \af for our final
experiment. Like this, we are able to show the absolute slowdown that \af
incurs by comparing with an unfiltered system, but also how \af compares to
industry-standard filtering tooling.

Each experiment was repeated three times and, as was the case for our Security
Analysis, all raw data, analysis scripts, and plots are available at the
repository linked in Appendix~\ref{apx:eval-artefacts}).

Each benchmark that we used is comprised of a set of microbenchmarks. In
\textcite{HeiserBenchmarkingCrimes}, Heiser, a respected researcher in the
\ac{os} community, argues that there is no good reason to report an arbitrary
subset of benchmark results. He goes further, and argues that this is not good
academic practice and mentions that he often rejects papers for committing this
``benchmarking crime''. 

Therefore, we've chosen to plot our figures with all benchmarking results
present, but only highlighting specific metrics measured in the benchmark. We
also only present our comparisons between \af and seccomp in cases where \af
incurs significant runtime overhead. The exception here is how in the \ac{npb}
benchmark results, where we report our comparison with the baseline. Our
reasoning for doing so is explained in Section~\todo{explain why we report diffs
with baseline for npb}.

This does make our results less clear to interpret than the Privilege Reduction
results in Table~\ref{tab:pr-results}, but we provide an explanation of all
results as they are presented. Some benchmarks also have many associated tests
and metrics with them (e.g. Redis), which makes highlighting everything in this
report infeasible. Therefore, we urge the reader to look for more detail in our
artefacts repository if they feel the need to do so.

As mentioned, we've also published all of our research artefacts to commit
to full academic transparency.

\subsubsection{\texttt{fio} Slowdown}

\begin{figure}[htbp]
    \centering
    % Subfigure 1: IOPS
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./evaluation-artefacts/results/fio/plots/fio_comparison_iops.pdf}
        \caption{IOPS comparison}
        \label{fig:fio-iops}
    \end{subfigure}
    \hfill % Space between horizontal figures
    % Subfigure 2: Latency
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./evaluation-artefacts/results/fio/plots/fio_comparison_latency.pdf}
        \caption{Average latency comparison}
        \label{fig:fio-latency}
    \end{subfigure}

    \medskip % Vertical space between rows
    
    % Subfigure 3: Percentage Difference
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./evaluation-artefacts/results/fio/plots/fio_comparison_percent_diff.pdf}
        \caption{Percentage difference overhead}
        \label{fig:fio-percdiff}
    \end{subfigure}
    % Add more subfigures if needed, adjust layout

    \caption{Performance evaluation for the Fio benchmark, comparing no filter (Baseline), Seccomp, and \af{}.} % Main figure caption
    \label{fig:fio-perf}
\end{figure}

% Discussion of Fio results, referencing Figure~\ref{fig:fio-perf}a, etc.


\subsubsection{Redis Slowdown}

\begin{figure}[htbp]
    \centering
    % Subfigure 1: RPS (Assuming one plot shows all comparisons)
    \begin{subfigure}[b]{\textwidth}
        \centering
        % Choose the best RPS comparison plot, e.g.:
        \includegraphics[width=\textwidth]{./evaluation-artefacts/results/redis/scmp-plots/redis_comparison_rps.pdf} 
        \caption{Requests Per Second (RPS)}
        \label{fig:redis-rps}
    \end{subfigure}
    \hfill
    % Subfigure 2: Latency (e.g., P99)
     \begin{subfigure}[b]{\textwidth}
        \centering
        % Choose the best latency plot, e.g.:
        \includegraphics[width=\textwidth]{./evaluation-artefacts/results/redis/scmp-plots/redis_comparison_p99_latency_ms.pdf} 
        \caption{P99 Latency (ms)}
        \label{fig:redis-p99}
    \end{subfigure}
    
    % Maybe add the percent diff plot if available and clear
     \medskip 
     \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./evaluation-artefacts/results/redis/scmp-plots/redis_comparison_percent_diff.pdf} % Check what this shows
        \caption{Percentage difference overhead}
        \label{fig:redis-percdiff}
    \end{subfigure}

    \caption{Performance evaluation for the Redis benchmark.}
    \label{fig:redis-perf}
\end{figure}

\subsubsection{\ac{npb} Slowdown}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.7 \textwidth}
        \centering
        \includegraphics[width=\textwidth]{./evaluation-artefacts/results/nasa/base-plots/npb_comparison_mops.pdf} 
        \caption{\ac{npb} Millions of Operations per Second (\af vs no filter)}
        \label{fig:npb-mops}
    \end{subfigure}
    \hfill
     \begin{subfigure}[b]{0.7 \textwidth}
        \centering
        % Choose the best latency plot, e.g.:
        \includegraphics[width=\textwidth]{./evaluation-artefacts/results/nasa/base-plots/npb_comparison_time.pdf} 
        \caption{\ac{npb} Time taken to run each microbenchmark (\af vs no
        filter)}
        \label{fig:npb-time}
    \end{subfigure}
     \medskip 
     \begin{subfigure}[b]{0.7 \textwidth}
        \centering
        \includegraphics[width=\textwidth]{./evaluation-artefacts/results/nasa/base-plots/npb_comparison_percent_diff.pdf} % Check what this shows
        \caption{Percentage difference overhead}
        \label{fig:npb-percdiff}
    \end{subfigure}

    \caption{Performance evaluation for the \ac{npb} benchmark suite.}
    \label{fig:npb-perf}
\end{figure}

\subsubsection{Postgres Slowdown}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.45 \textwidth}
        \centering
        \includegraphics[width=\textwidth]{./evaluation-artefacts/results/postgres/plots/pgbench_comparison_tps.pdf} 
        \caption{Postgres: Comparison of transactions per second with \af
        and seccomp}
        \label{fig:pgs-txs}
    \end{subfigure}
    \hfill
     \begin{subfigure}[b]{0.45 \textwidth}
        \centering
        % Choose the best latency plot, e.g.:
        \includegraphics[width=\textwidth]{./evaluation-artefacts/results/postgres/plots/pgbench_comparison_latency.pdf} 
        \caption{Postgres: Comparison of average latency with \af versus
        seccomp}
        \label{fig:pgs-time}
    \end{subfigure}
     \medskip 
     \begin{subfigure}[b]{0.45 \textwidth}
        \centering
        \includegraphics[width=\textwidth]{./evaluation-artefacts/results/postgres/plots/pgbench_comparison_percent_diff.pdf} % Check what this shows
        \caption{Percentage difference overhead between \af and seccomp}
        \label{fig:pgs-percdiff}
    \end{subfigure}

    \caption{Performance evaluation for the Postgres benchmark.}
    \label{fig:pgs-perf}
\end{figure}

\subsubsection{Nginx Slowdown}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.9 \textwidth}
        \centering
        \includegraphics[width=\textwidth]{./evaluation-artefacts/results/nginx/plots/wrk_comparison_requests_per_second.pdf} 
        \caption{Nginx: comparison of transfers per second between \af and
        seccomp.}
        \label{fig:nginx-rps}
    \end{subfigure}
    \hfill
     \begin{subfigure}[b]{0.45 \textwidth}
        \centering
        % Choose the best latency plot, e.g.:
        \includegraphics[width=\textwidth]{./evaluation-artefacts/results/nginx/plots/wrk_comparison_latency.pdf} 
        \caption{Nginx: Comparison of average latency with \af versus
        seccomp}
        \label{fig:nginx-time}
    \end{subfigure}
     \medskip 
     \begin{subfigure}[b]{0.45 \textwidth}
        \centering
        \includegraphics[width=\textwidth]{./evaluation-artefacts/results/nginx/plots/wrk_comparison_percent_diff.pdf} % Check what this shows
        \caption{Percentage difference overhead between \af and seccomp}
        \label{fig:nginx-percdiff}
    \end{subfigure}

    \caption{Performance evaluation for the Nginx benchmark.}
    \label{fig:nginx-perf}
\end{figure}


